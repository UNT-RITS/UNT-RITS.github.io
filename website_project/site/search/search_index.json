{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"COVID-19 Response Update Due to UNT\u2019s response to the COVID-19 outbreak, NTSC, HPC and DSA, are offering mostly online support services during this time. Walk-in and in-person appointments are limited to favor virtual appointments via Zoom, phone, or other means of online communication. Please send an email to SciComp-Support@unt.edu to schedule an appointment with a NTSC staff member. Thank you for your understanding. Recent Updates 11/2020 New Julia version 1.5.2 Check out info for Nov TUG meeting Welcome \u00b6 Welcome to the North Texas Scientific Computing ! NTSC supports the High Performance Computing resources at the University of North Texas . About this Documentation \u00b6 This documentation is the official documentation of NTSC\u2019s flagship resource, Talon3. Please contact SciComp-Support@unt.edu for any Talon3 and UNT HPC support questions! You can find information on getting a HPC Talon3 account here Make sure you check out \u00b6 UNT HPC Website NTSC on Twitter NTSC on GitHub Talon System Stats \u00b6","title":"About"},{"location":"#welcome","text":"Welcome to the North Texas Scientific Computing ! NTSC supports the High Performance Computing resources at the University of North Texas .","title":"Welcome"},{"location":"#about-this-documentation","text":"This documentation is the official documentation of NTSC\u2019s flagship resource, Talon3. Please contact SciComp-Support@unt.edu for any Talon3 and UNT HPC support questions! You can find information on getting a HPC Talon3 account here","title":"About this Documentation"},{"location":"#make-sure-you-check-out","text":"UNT HPC Website NTSC on Twitter NTSC on GitHub","title":"Make sure you check out"},{"location":"#talon-system-stats","text":"","title":"Talon System Stats"},{"location":"login/","text":"Logging into Talon3 \u00b6 Talon3 is a HPC computing cluster that users remotly access. To login, you will need to make a remote connection to one of Talon 3\u2019s login nodes. User MUST : Have a ACTIVE Talon account that has been approved by NTSC Here is more information about requesting a Talon3 account Be connected to the UNT network This includes computers connected to the UNT Campus LAN, Eaglenet Wi-Fi, and the UNT VPN Network Talon3 can be access with the following methods Secure Shell (SSH) terminal connection Rstudio server Jupyter Hub PyCharm Login \u00b6 In order to login to talon you will have to use SSH command on your personal computer\u2019s temrinal : $ ssh euid123@talon3.hpc.unt.edu Then it will ask for you password. To make things easier you cna have it save the password and even create a shortcut so you can speed things up: Setup alias on you SSH connection \u00b6 Make sure you are in your home directory: $ cd ~ Check if you have a folder names .ssh $ ls -a If you don\u2019t you have to create one: $ mkdir .ssh Now you need to create a file named config. Use Nano or Vim or any other editor you are comfortable with: $ vim ~/.ssh/config If, for example, you want to create a shortcut to ssh on Talon3, your config file should look like: 1 ```bash Host t3 HostName talon3.hpc.unt.edu User euid123 Port 22 Where t3 (it can be any word you want) is the shortcut you will use instead of typing: bash $ ssh euid123@talon3.hpc.unt.edu ``` You will type: $ ssh t3 Save SSH password \u00b6 Now you have a shortcut, but you will still need to enter your password every time. In order to save your passwords you will have to create a keygen file: $ ssh-keygen When prompted with this: Enter file in which to save the key ( /home/your_user/.ssh/id_rsa ) : Just press Enter to save the id_ras in the path. Then you will be prompted with: Enter passphrase ( empty for no passphrase ) : Type Enter again if you don\u2019t want to type any password when ssh Enter same passphrase again: Hit Enter again! You should see: Your identification has been saved in /home/your_user/.ssh/id_rsa. Your public key has been saved in /home/you_user/.ssh/id_rsa.pub. Now you can start saving passwords on you SSH: $ ssh-copy-id euid123@talon3.hpc.unt.edu or if you have your shortcut $ ssh-copy-id t3 You will be prompted to enter password. It will logout, and you should see: Number of key ( s ) added: 1 Now try logging into the machine, with: \"ssh 't3'\" and check to make sure that only the key ( s ) you wanted were added. Congratulations! Now every time you need to login to Talon you just need to type ssh t3 and your good to go! When you want to copy files using SCP \u00b6 If you are not familiar with SCP you can find a nice example here Now since you setup your ssh shortcut to be t3 and the password saved, when you use SCP it will be a lot easier! Just type scp path/to/file/my_file t3:. It will automatically login and enter the password for you! For more details on how to transfer files to Talon use Transferring files","title":"Logging into Talon3"},{"location":"login/#logging-into-talon3","text":"Talon3 is a HPC computing cluster that users remotly access. To login, you will need to make a remote connection to one of Talon 3\u2019s login nodes. User MUST : Have a ACTIVE Talon account that has been approved by NTSC Here is more information about requesting a Talon3 account Be connected to the UNT network This includes computers connected to the UNT Campus LAN, Eaglenet Wi-Fi, and the UNT VPN Network Talon3 can be access with the following methods Secure Shell (SSH) terminal connection Rstudio server Jupyter Hub PyCharm","title":"Logging into Talon3"},{"location":"login/#login","text":"In order to login to talon you will have to use SSH command on your personal computer\u2019s temrinal : $ ssh euid123@talon3.hpc.unt.edu Then it will ask for you password. To make things easier you cna have it save the password and even create a shortcut so you can speed things up:","title":"Login"},{"location":"login/#setup-alias-on-you-ssh-connection","text":"Make sure you are in your home directory: $ cd ~ Check if you have a folder names .ssh $ ls -a If you don\u2019t you have to create one: $ mkdir .ssh Now you need to create a file named config. Use Nano or Vim or any other editor you are comfortable with: $ vim ~/.ssh/config If, for example, you want to create a shortcut to ssh on Talon3, your config file should look like: 1 ```bash Host t3 HostName talon3.hpc.unt.edu User euid123 Port 22 Where t3 (it can be any word you want) is the shortcut you will use instead of typing: bash $ ssh euid123@talon3.hpc.unt.edu ``` You will type: $ ssh t3","title":"Setup alias on you SSH connection"},{"location":"login/#save-ssh-password","text":"Now you have a shortcut, but you will still need to enter your password every time. In order to save your passwords you will have to create a keygen file: $ ssh-keygen When prompted with this: Enter file in which to save the key ( /home/your_user/.ssh/id_rsa ) : Just press Enter to save the id_ras in the path. Then you will be prompted with: Enter passphrase ( empty for no passphrase ) : Type Enter again if you don\u2019t want to type any password when ssh Enter same passphrase again: Hit Enter again! You should see: Your identification has been saved in /home/your_user/.ssh/id_rsa. Your public key has been saved in /home/you_user/.ssh/id_rsa.pub. Now you can start saving passwords on you SSH: $ ssh-copy-id euid123@talon3.hpc.unt.edu or if you have your shortcut $ ssh-copy-id t3 You will be prompted to enter password. It will logout, and you should see: Number of key ( s ) added: 1 Now try logging into the machine, with: \"ssh 't3'\" and check to make sure that only the key ( s ) you wanted were added. Congratulations! Now every time you need to login to Talon you just need to type ssh t3 and your good to go!","title":"Save SSH password"},{"location":"login/#when-you-want-to-copy-files-using-scp","text":"If you are not familiar with SCP you can find a nice example here Now since you setup your ssh shortcut to be t3 and the password saved, when you use SCP it will be a lot easier! Just type scp path/to/file/my_file t3:. It will automatically login and enter the password for you! For more details on how to transfer files to Talon use Transferring files","title":"When you want to copy files using SCP"},{"location":"newaccount/","text":"New User Account Information \u00b6 HPC accounts \u00b6 An HPC account gives you access to UNT\u2019s HPC resources. This includes Talon, UNT\u2019s flagship supercomputing cluster with over 8,300 CPU cores, over 1.4 PB of storage space, and 150,000 GPU-accelerated cores. With this account, you can login into the Talon3 computing resource. HPC consulting services are offered to anyone from members of our highly skilled HPC staff members. This includes any help using the HPC resources for their research needs. Typically, new accounts are create and added to a Talon3 allocation . What are Talon allocations and what types of allocations are available? \u00b6 Talon allocations are requests to use Talon3 resources via the SLURM queuing system. They control the amount of resources researcher can use and the priority of each job. When a Talon account is created, it is added to a NEW or EXISTING allocation . On Talon, there are two types of allocations, Research and Academic . Info ONLY faculty and staff can request a NEW allocation with applying for a Talon account. Students and Post-Docs MUST request an EXSITING account from a faculty staff member that already has an Talon account/allocation Research \u00b6 Research allocations are given to faculty or staff PI\u2019s to conduct any project that require access to HPC resources. Any faculty or staff PI can request an Research allocation when filling out an Account Request form online. All requests go through approval by the HPC staff and typical accounts and allocations are awarded within 1-2 business days. A small allocation proposal that details the type of research that will be performed using UNT HPC resources and any external funding and grants relating to the research. Research allocations are valid until the end of the fiscal year regardless of when the allocation was created. At the end of each fiscal year, each Research allocation is audited to assess if the allocation is renewed for the following year. Typically, this involves the HPC staff contacting the PI, confirming their account as active and the users under their allocation are active and submitting a new research proposal and listing any publications and presentations that were used with UNT HPC resources. Confirmation is required to keep their allocation active as well as the users under their allocation. Any allocation/account that become inactive can contact SciComp-Support@unt.edu to reenable their allocation. Typically, a PI\u2019s research is funded by the PI\u2019s grant awards. The dollar amount generated by a PI\u2019s external grants will determinate initial priority within the Talon 3 job queueing system. Students and Post-Docs cannot request a new Research Allocation, but they can be added to an already existing Research allocation, headed by a faculty or staff PI. Students and Post-Docs requesting HPC accounts will need to list a faculty or staff PI Sponsor when filling out an Account Request form. Once a request is made, the PI of the Research allocation will be contacted for approval. Then, the student/Post-Doc will have their account created under their Sponsor\u2019s Research Allocation. NOTE: The PI/Sponsor of the Research allocation will be responsible for any activity of all users using the allocation. The PI/Sponsor can request to disable a user\u2019s access to the allocation at any time or request access to any data generated by any user under their allocation. Academic \u00b6 Academic allocations are awarded to UNT class instructors for a course that can make use of HPC resources. These allocations are directed by the instructors and students taking the course can be allocated accounts on Talon. The allocations are only active for the duration of the course. If an instructor wants access to HPC resources for a course, please contact SciComp-Support@unt.edu at least one month before the course starts to ensure the Academic allocations are ready when the course starts. As an added service provided to Academic allocations, a member of the HPC or DSA staff can take over a class if the instructor is out for any reason (i.e. out of town meeting or conference). Instead of canceling class, we can provide a lecture over various HPC, programing, and statistical methodology topics. The HPC staff can also give a Talon 3 User Orientation to a class especially if students will heavily use Talon 3 during their coursework. Please contact the HPC staff in advance if you would like to use this service. Who is eligible for an account/allocation? \u00b6 Any UNT faculty, students, postdocs, research fellows, and external visiting scholars is eligible for an account to UNT\u2019s HPC resources. Principal Investigators (PI) can receive allocations with their account request to run their calculations on the computing resources. All other users (i.e. students, external scholar, etc.) can also apply for an account but must be sponsored by a UNT faculty-level PI. PIs are responsible for ensuring that they and sponsored users comply with all relevant, University policies, and State and Federal laws. Every new HPC account holder MUST have an active UNT AMS EUID to apply. If you are an external visiting scholar/collaborator you can apply for a UNT AMS account (EUID) through the Office of the Provost and Vice President for Academic Affairs by filling out the Visiting Scholar/Researcher EMPLID/EUID Request Form (VPAA-40a) and following directions from the Office of the Provost. Accounts CANNOT be shared with anyone. Each user requires an individual account. How do I apply for an account? \u00b6 In order to request your Talon3 account, click here To access this form, you are required to enter valid UNT credentials, your EUID and AMS password.","title":"Talon3 Account Information"},{"location":"newaccount/#new-user-account-information","text":"","title":"New User Account Information"},{"location":"newaccount/#hpc-accounts","text":"An HPC account gives you access to UNT\u2019s HPC resources. This includes Talon, UNT\u2019s flagship supercomputing cluster with over 8,300 CPU cores, over 1.4 PB of storage space, and 150,000 GPU-accelerated cores. With this account, you can login into the Talon3 computing resource. HPC consulting services are offered to anyone from members of our highly skilled HPC staff members. This includes any help using the HPC resources for their research needs. Typically, new accounts are create and added to a Talon3 allocation .","title":"HPC accounts"},{"location":"newaccount/#what-are-talon-allocations-and-what-types-of-allocations-are-available","text":"Talon allocations are requests to use Talon3 resources via the SLURM queuing system. They control the amount of resources researcher can use and the priority of each job. When a Talon account is created, it is added to a NEW or EXISTING allocation . On Talon, there are two types of allocations, Research and Academic . Info ONLY faculty and staff can request a NEW allocation with applying for a Talon account. Students and Post-Docs MUST request an EXSITING account from a faculty staff member that already has an Talon account/allocation","title":"What are Talon allocations and what types of allocations are available?"},{"location":"newaccount/#research","text":"Research allocations are given to faculty or staff PI\u2019s to conduct any project that require access to HPC resources. Any faculty or staff PI can request an Research allocation when filling out an Account Request form online. All requests go through approval by the HPC staff and typical accounts and allocations are awarded within 1-2 business days. A small allocation proposal that details the type of research that will be performed using UNT HPC resources and any external funding and grants relating to the research. Research allocations are valid until the end of the fiscal year regardless of when the allocation was created. At the end of each fiscal year, each Research allocation is audited to assess if the allocation is renewed for the following year. Typically, this involves the HPC staff contacting the PI, confirming their account as active and the users under their allocation are active and submitting a new research proposal and listing any publications and presentations that were used with UNT HPC resources. Confirmation is required to keep their allocation active as well as the users under their allocation. Any allocation/account that become inactive can contact SciComp-Support@unt.edu to reenable their allocation. Typically, a PI\u2019s research is funded by the PI\u2019s grant awards. The dollar amount generated by a PI\u2019s external grants will determinate initial priority within the Talon 3 job queueing system. Students and Post-Docs cannot request a new Research Allocation, but they can be added to an already existing Research allocation, headed by a faculty or staff PI. Students and Post-Docs requesting HPC accounts will need to list a faculty or staff PI Sponsor when filling out an Account Request form. Once a request is made, the PI of the Research allocation will be contacted for approval. Then, the student/Post-Doc will have their account created under their Sponsor\u2019s Research Allocation. NOTE: The PI/Sponsor of the Research allocation will be responsible for any activity of all users using the allocation. The PI/Sponsor can request to disable a user\u2019s access to the allocation at any time or request access to any data generated by any user under their allocation.","title":"Research"},{"location":"newaccount/#academic","text":"Academic allocations are awarded to UNT class instructors for a course that can make use of HPC resources. These allocations are directed by the instructors and students taking the course can be allocated accounts on Talon. The allocations are only active for the duration of the course. If an instructor wants access to HPC resources for a course, please contact SciComp-Support@unt.edu at least one month before the course starts to ensure the Academic allocations are ready when the course starts. As an added service provided to Academic allocations, a member of the HPC or DSA staff can take over a class if the instructor is out for any reason (i.e. out of town meeting or conference). Instead of canceling class, we can provide a lecture over various HPC, programing, and statistical methodology topics. The HPC staff can also give a Talon 3 User Orientation to a class especially if students will heavily use Talon 3 during their coursework. Please contact the HPC staff in advance if you would like to use this service.","title":"Academic"},{"location":"newaccount/#who-is-eligible-for-an-accountallocation","text":"Any UNT faculty, students, postdocs, research fellows, and external visiting scholars is eligible for an account to UNT\u2019s HPC resources. Principal Investigators (PI) can receive allocations with their account request to run their calculations on the computing resources. All other users (i.e. students, external scholar, etc.) can also apply for an account but must be sponsored by a UNT faculty-level PI. PIs are responsible for ensuring that they and sponsored users comply with all relevant, University policies, and State and Federal laws. Every new HPC account holder MUST have an active UNT AMS EUID to apply. If you are an external visiting scholar/collaborator you can apply for a UNT AMS account (EUID) through the Office of the Provost and Vice President for Academic Affairs by filling out the Visiting Scholar/Researcher EMPLID/EUID Request Form (VPAA-40a) and following directions from the Office of the Provost. Accounts CANNOT be shared with anyone. Each user requires an individual account.","title":"Who is eligible for an account/allocation?"},{"location":"newaccount/#how-do-i-apply-for-an-account","text":"In order to request your Talon3 account, click here To access this form, you are required to enter valid UNT credentials, your EUID and AMS password.","title":"How do I apply for an account?"},{"location":"stuff/","text":"Welcome to Talon Hight-Performance Computing Documentation \u00b6 BTW this is mobile friendly and easy to deploy on GitHub Pages http://UNT-RITS.github.io/talon_sop/_build/html/index.html Task List \u00b6 Lorem ipsum dolor sit amet, consectetur adipiscing elit Nulla lobortis egestas semper Curabitur elit nibh, euismod et ullamcorper at, iaculis feugiat est Vestibulum convallis sit amet nisi a tincidunt In hac habitasse platea dictumst In scelerisque nibh non dolor mollis congue sed et metus Sed egestas felis quis elit dapibus, ac aliquet turpis mattis Praesent sed risus massa Aenean pretium efficitur erat, donec pharetra, ligula non scelerisque Nulla vel eros venenatis, imperdiet enim id, faucibus nisi Bundle code together \u00b6 Bash #!/bin/bash echo \"Hello world!\" C #include <stdio.h> int main ( void ) { printf ( \"Hello world! \\n \" ); return 0 ; } C++ #include <iostream> int main ( void ) { std :: cout << \"Hello world!\" << std :: endl ; return 0 ; } C# using System ; class Program { static void Main ( string [] args ) { Console . WriteLine ( \"Hello world!\" ); } } Python Starter \u00b6 Content \u00b6 1 2 3 4 import tensorflow as tf import os print ( \"Hello world\" ) 1 2 3 4 import tensorflow as tf import os print ( \"Hello world\" ) Formulas \u00b6 \\frac{n!}{k!(n-k)!} = \\binom{n}{k} \\frac{n!}{k!(n-k)!} = \\binom{n}{k} \u2013 we can use Material Design icons \u2013 we can also use FontAwesome icons :octicons-octoface: \u2013 that\u2019s not all, we can also use GitHub\u2019s Octicons Foot Note \u00b6 Lorem ipsum 1 dolor sit amet R Starter \u00b6 Content \u00b6 Job Submit \u00b6 Content \u00b6 Tables \u00b6 Fruit List Apple Banana Kiwi Fruit Table Fruit Color Apple Red Banana Yellow Kiwi Green Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. \u21a9","title":"Welcome to Talon Hight-Performance Computing Documentation"},{"location":"stuff/#welcome-to-talon-hight-performance-computing-documentation","text":"BTW this is mobile friendly and easy to deploy on GitHub Pages http://UNT-RITS.github.io/talon_sop/_build/html/index.html","title":"Welcome to Talon Hight-Performance Computing Documentation"},{"location":"stuff/#task-list","text":"Lorem ipsum dolor sit amet, consectetur adipiscing elit Nulla lobortis egestas semper Curabitur elit nibh, euismod et ullamcorper at, iaculis feugiat est Vestibulum convallis sit amet nisi a tincidunt In hac habitasse platea dictumst In scelerisque nibh non dolor mollis congue sed et metus Sed egestas felis quis elit dapibus, ac aliquet turpis mattis Praesent sed risus massa Aenean pretium efficitur erat, donec pharetra, ligula non scelerisque Nulla vel eros venenatis, imperdiet enim id, faucibus nisi","title":"Task List"},{"location":"stuff/#bundle-code-together","text":"Bash #!/bin/bash echo \"Hello world!\" C #include <stdio.h> int main ( void ) { printf ( \"Hello world! \\n \" ); return 0 ; } C++ #include <iostream> int main ( void ) { std :: cout << \"Hello world!\" << std :: endl ; return 0 ; } C# using System ; class Program { static void Main ( string [] args ) { Console . WriteLine ( \"Hello world!\" ); } }","title":"Bundle code together"},{"location":"stuff/#python-starter","text":"","title":"Python Starter"},{"location":"stuff/#content","text":"1 2 3 4 import tensorflow as tf import os print ( \"Hello world\" ) 1 2 3 4 import tensorflow as tf import os print ( \"Hello world\" )","title":"Content"},{"location":"stuff/#formulas","text":"\\frac{n!}{k!(n-k)!} = \\binom{n}{k} \\frac{n!}{k!(n-k)!} = \\binom{n}{k} \u2013 we can use Material Design icons \u2013 we can also use FontAwesome icons :octicons-octoface: \u2013 that\u2019s not all, we can also use GitHub\u2019s Octicons","title":"Formulas"},{"location":"stuff/#foot-note","text":"Lorem ipsum 1 dolor sit amet","title":"Foot Note"},{"location":"stuff/#r-starter","text":"","title":"R Starter"},{"location":"stuff/#content_1","text":"","title":"Content"},{"location":"stuff/#job-submit","text":"","title":"Job Submit"},{"location":"stuff/#content_2","text":"","title":"Content"},{"location":"stuff/#tables","text":"Fruit List Apple Banana Kiwi Fruit Table Fruit Color Apple Red Banana Yellow Kiwi Green Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. \u21a9","title":"Tables"},{"location":"under_construction/","text":"Failure We are still working on updating our docs! Check back for more information! Please email SciComp-Support@unt.edu for your questions!","title":"Compiling Procedure"},{"location":"details/details/","text":"Talon Hight-Performance Computing \u00b6 Terminology \u00b6 We refer to the same thing when we say: Talon , HPC , Talon3 (refering to the version of Talon), Cluster , High-Performance Computing , Server . Compute Node \u00b6 The Talon 3 compute nodes are where the users will run their software and other computationally intensive applications. Users do NOT directly login to these nodes . Users run their calculations on these nodes by submitting them to the Slurm queuing system. These specific nodes can be chosen in the SLURM queuing system by including a resource request option #SBATCH -C to meet your needs (i.e. for the Dell PowerEdge C6320 use #SBATCH -C c6320 ): Quanity Memory(GB) Cores Description 192 64 GB 28 Dell PowerEdge C6320 server with two 2.4GHz Intel Xeon E5-2680 v4 fourteen-core processors. 75 32 GB 16 Dell PowerEdge R420 server with two 2.1GHz Intel Xeon E5-2450 eight-core processors. 64 64 GB 16 Dell PowerEdge R420 server with two 2.1GHz Intel Xeon E5-2450 eight-core processors 8 512 GB 32 Dell PowerEdge R720 server with four 2.4GHz Intel Xeon E5-4640 eight-core processors. 16 64 GB 28 Dell PowerEdge R730 server with two 2.4GHz Intel Xeon E5-2680 v4 fourteen-core processors and two Nvidia Tesla K80 GPUS ( 4,992 GPU cores/card) . Login Node \u00b6 The Talon 3 login nodes can be access by the domain name: talon3.hpc.unt.edu These login nodes are for users to setup jobs and simple file editing via a Linux Command Line terminal. Users will submit jobs via the SLURM queuing system to be dispatched on the compute nodes. There is NO X11 Forwarding capable on this server. Visualization Login Nodes \u00b6 There are five login nodes that have X11 capabilities and are Slurm submission hosts. This host is intended for using the graphical-based software, e.g., gnuplot, matplotlib, and other notebook features in software, such as MATLAB and Mathematica. Users can use these nodes for any post-processing tasks, but any compute-intensive tasks MUST be submitted through the Slurm queuing system. Also, X11 graphical sessions require large bandwidth to work effectively, and you may experience lag while on a home DSL or Cable ISP. X11 forwarding will need to be enabled with the SSH client that you are using. Alternately, you can access a specific visualization node by entering one of the following domain names: vis.acs.unt.edu vis-01.acs.unt.edu vis-02.acs.unt.edu vis-03.acs.unt.edu vis-04.acs.unt.edu GPU Node \u00b6 There are 16 GPU Nodes on Talon. Each one contains 2 Nvidia Tesla K80 GPUs. Quanity Memory(GB) Cores Description 16 64 GB 28 Dell PowerEdge R730 server with two 2.4GHz Intel Xeon E5-2680 v4 fourteen-core processors and two Nvidia Tesla K80 GPUS ( 4,992 GPU cores/card) . Job \u00b6 In an HPC cluster, the users\u2019 tasks to be done on compute nodes are controlled by a batch queuing system. Queuing systems manage job requests (shell scripts generally referred to as jobs) submitted by all users on Talon 3. In other words, to get your computations done by the cluster, you must submit a job request to a specific batch queue. The scheduler will assign your job to a compute node in the order determined by the policy on that queue and the availability of an idle compute node. Currently, Talon 3 resources have several policies in place to help guarantee fair resource utilization from all users. Batch / batch job \u00b6 Refers to the process of creating a .job file and submit it to HPC using the slurm command sbatch job_name.job where job_name.job is your job file. Here is a brief example of a .job file that runs a python script named test.py : #!/bin/bash #SBATCH -J job_name #SBATCH -o output_job.o%j #SBATCH -e error_job.e%j #SBATCH -p public #SBATCH --qos general #SBATCH -N 1 #SBATCH -n 1 #SBATCH -C c6320 module load python python test.py For more details on how to write a job file please check: Slurm Tutotials Interractive Session \u00b6 Interactive job sessions can be used on Talon if you need to compile or test software. An example command of starting an interactive sessions is shown below: $ srun -p public --qos general -C c6320 --pty bash This launches an interactive job session and lanches a bash shell to a compute node. From there, you can execute software and shell commands that would otherwise not be allowed on the Talon login nodes. You can use same Slurm Commands : Check our UNT website Example of requesting 1 node with email notificaiton for CPU: srun -p public --qos general --mail-user = user@unt.edu --mail-type = ALL -N 1 -C c6320 --pty bash Example of requesting 1 node with email notificaiton for GPU: srun -p gpu --mail-user = user@unt.edu --mail-type = ALL -N 1 --pty bash This is very useful because it notifies you when your node has been allocated to you so you can start work! For more details on how to write a job file please check: Slurm Tutotials Module/s \u00b6 Talon 3 uses LMOD to manage your environment. It handles setting up access to the software packages, libraries, and other utilities. To see the list of all available modules, run the command: module avail To see which modules are already loaded: module list To add a module for the current session: module load <module_name> where is the name of the module you wish to load. For example, if you need to use the Intel compilers, you can run module load python/3.6.5 This will load the Python version 3.6.5. Here is a table of current modules that can be found on Talon 3: /cm/shared/modulefiles/science /cm/shared/modulefiles/science /cm/shared/modulefiles/utils JDFTx/1.5.0 R/R-devel PackageEnv/gcc4.8.5_OBLAS0.2.19_OMPI2.1.0 OpenSees/3.0.3 R/3.6.0 (D) PackageEnv/gcc8.1.0_OBLAS0.2.19_OMPI2.1.0 amber/16-cuda-mpi anaconda/5.2 PackageEnv/intel17.0.4_MKL_IMPI_AVX amber/16-gen atlas/3.10.3 PackageEnv/intel17.0.4_MKL_IMPI_AVX2 amber/18-gen (D) bison/3.5.90 PackageEnv/intel18.0.3_gcc4.8.5_MKL_IMPI_AVX ansys/2019r1 boost/1.63.0 PackageEnv/intel18.0.3_gcc4.8.5_MKL_IMPI_AVX2 armadillo/7.950.1 boost/1.71.0 (D) PackageEnv/intel18.0.3_gcc8.1.0_MKL_IMPI_AVX (D) bamtools/2.4.1 bzip2/1.0.6 bazel/0.9.0 cluster-tools/7.3 bazel/0.14.0 (D) cmake/3.8.0 bbmap/38.76 cmake/3.15.2 (D) bcftools/1.4 cmd bcftools/1.4.1 cmgui/7.3 bcftools/1.6 (D) cmsh beast/1.8.4 cuda/101/toolkit/10.1.243 beast/2.4.5 (D) cuda/102/toolkit/10.2.89 bedtools/2.18 cuda/75/blas/7.5.18 bioperl/1.007002 cuda/75/fft/7.5.18 blast/2.6.0 cuda/75/nsight/7.5.18 bowtie2/2.3.1 cuda/75/profiler/7.5.18 bowtie2/2.3.2 (D) cuda/75/toolkit/7.5.18 bsr/bsr cuda/80/blas/8.0.61 bsr/dbsrhf (D) cuda/80/fft/8.0.61 bwa/0.7.17 cuda/80/nsight/8.0.61 caffe/cpu cuda/80/profiler/8.0.61 caffe/gpu (D) cuda/80/toolkit/8.0.61 cairo/1.14.10 cuda/90/blas/9.0.176 cd-hit/4.6.6 cuda/90/fft/9.0.176 cd-hit/4.8.1 (D) cuda/90/nsight/9.0.176 circos/0.69 cuda/90/profiler/9.0.176 clark/1.2.4 cuda/90/toolkit/9.0.176 comsol/5.5 cuda/91/blas/9.1.85 cp2k/6.1 cuda/91/fft/9.1.85 cufflinks/2.2.1 cuda/91/nsight/9.1.85 ddocent/2.2.16 cuda/91/profiler/9.1.85 dirac/2019 cuda/91/toolkit/9.1.85 dlpoly/class-1.10 cudnn/5.1/cuda75 dlpoly/4.08 (D) cudnn/5.1/cuda80 (D) eigen/3.3.3 cudnn/6.0/cuda75 emto/5.8.1 cudnn/6.0/cuda80 (D) espresso/6.0 cudnn/7.0/cuda90 espresso/6.5 (D) cudnn/7.1/cuda90 fastqc/0.11.8 cudnn/7.6.5/cuda90 fastxtoolkit/0.0.13 cudnn/7.6.5/cuda101 fpart/1.0.0 cudnn/7.6.5/cuda102 (D) gamess/08182016 curl/7.54.1 gamess/10302017 (D) ea-utils/2017 gatk/3.80 fftw/2.1.5 gaussian/g09-revA fftw/3.3.6 gaussian/g09-revD (D) fftw/3.3.8 (D) gaussian/g16-RevA.03-ax2 gcc/4.8.5 genenetwork/2 gcc/5.5.0 gistic2/2.0.23 gcc/6.1.0 gnuplot/5.0.6 (D) gcc/6.3.0 grasp/grasp2018 gcc/8.1.0 (L,D) gromacs/5.1.4-fftw git/2.9.3 gromacs/2019.1-cuda glibc/2.23 gromacs/2019.1 (D) gnuplot/5.0.6 gsl/2.4 go/1.10.3 gulp/4.5 gperf/3.1 h2o4pu/0.3.2 gsl/2.6 (D) hadoop/2.8.0-rdma hdf5/1.6.10 hisat/0.1.6 hdf5/1.8.18-gnu hisat/2.0.5 (D) hdf5/1.8.18-intel icu/4c-59 hdf5/1.10.5 (D) jags/4.2.0 hisat/0.1.6 kallisto/0.43.1 hisat/2.0.5 keras/2.1.6 htslib/1.4 keras/2.2.0 (D) htslib/1.6 (D) kraken/0.10 htslib/1.10 kraken/2.0.7 (D) intel/IMPI/17.0.4-AVX lammps/7Aug19 intel/IMPI/17.0.4-AVX2 lammps/11Aug17 intel/IMPI/17.0.4 lammps/17Nov16 (D) intel/IMPI/18.0.3-AVX maps/4.2 intel/IMPI/18.0.3-AVX2 mathematica/10.0 intel/IMPI/18.0.3 (D) mathematica/11.3 (D) intel/compilers/17.0.4-AVX matlab/R2014b intel/compilers/17.0.4-AVX2 matlab/R2016b intel/compilers/18.0.3-AVX (D) matlab/R2018b (D) intel/compilers/18.0.3-AVX2 minimac2/2014.9.15 intel/mkl/17.0.4-AVX mothur/1.43.0 intel/mkl/17.0.4-AVX2 mpiblast/1.6.0 intel/mkl/17.0.4 mplus/8 intel/mkl/18.0.3-AVX mummer/4.0 intel/mkl/18.0.3-AVX2 namd/2.12-cuda intel/mkl/18.0.3 (D) namd/2.12 (D) java/1.8 ncurses/6.0 java/13.0.2 (D) nonmem/7.4.3 jupyter/4.4.0 nwchem/6.8 lapack/3.7.0 nwchem/6.8.1-dev (D) libffi/3.2.1 oases/0.2.8 libtool/2.4.6 oncosnp/1.4 miniconda/2/4.7.12 openfoam/5.0 miniconda/\u00be.7.12 ovito/2.9.0 netcdf/c/4.7.3 paraview/5.3.0 octave/5.2.0 parsync/1.67 openblas/0.2.19 phyml/3.1 opencv/3.4.7 picard/2.10 openmpi/gcc/1.10.6 plink/1.07 openmpi/gcc/2.1.0 plink/1.90 openmpi/gcc/3.1.4 (D) plink/2.00 (D) openmpi/intel/2.1.0-cuda psi4/1.0.0 openmpi/intel/2.1.0 (D) pytorch/gpu-1.1.0 openmpi/pgi/1.10.2 pytorch/gpu-1.2.0 openmpi/pgi/2.1.2 pytorch/gpu-1.4.0 openmpi/pgi/3.1.3 (D) pytorch/gpu-1.5.0 (D) parallel/20200122 qiime2/2020.2 perl/5.24.1 raxml/8.2.10 (D) perl/5.30.2 (D) rohan/1.0 pgi/17.5 sagemath/7.6 pgi/18.10 salmon/1.0 pgi/19.7 (D) samtools/1.4 pssh/2.3.1 samtools/1.4.1 pyMPI/2.4 samtools/1.6 (D) python/2.7.13 seqprep/1.0 python/2.7.14-pyCUDA sklearn/0.18.1 python/3.5.7 soapdenovo/1.03 python/3.6.0 spades/3.10.1 python/3.6.0-2 spades/3.12.0 (D) python/3.6.5 spark/2.3.0 python/3.7.0 spring/spring5 python/3.7.4 (D) sra-tools/2.92 qt/5.14.0 stacks/1.46 raxml/8.2.10 star/2.5.3a readline/7.0 stringtie/1.3.3b singularity/3.6 subread/2.0.0 slurm/16.05.8 (L) tdep/1.1 sparsehash/2.0.2 tecplot/2018R1 szip/1.12b tensorflow/1.2.0 tcl/8.6.7 tensorflow/1.10.1-gpu xalt/2.6 tensorflow/1.10.1 xalt/2.8 (L,D) tensorflow/1.12.3-gpu zlib/1.2.11 tensorflow/2.0 tensorflow/2.1.0-gpu (D) tophat/2.1.1 transformers/transformers trinity/2.4.0 vasp/4.6-vtst vasp/5.4.1-vtst vasp/5.4.1 vasp/5.4.4-vtst (D) vcftools/0.1.17 velvet/1.2.10 visit/2.12.2 vmd/1.9.3 warp/4.5 xoopic/2.70","title":"Talon Hight-Performance Computing"},{"location":"details/details/#talon-hight-performance-computing","text":"","title":"Talon Hight-Performance Computing"},{"location":"details/details/#terminology","text":"We refer to the same thing when we say: Talon , HPC , Talon3 (refering to the version of Talon), Cluster , High-Performance Computing , Server .","title":"Terminology"},{"location":"details/details/#compute-node","text":"The Talon 3 compute nodes are where the users will run their software and other computationally intensive applications. Users do NOT directly login to these nodes . Users run their calculations on these nodes by submitting them to the Slurm queuing system. These specific nodes can be chosen in the SLURM queuing system by including a resource request option #SBATCH -C to meet your needs (i.e. for the Dell PowerEdge C6320 use #SBATCH -C c6320 ): Quanity Memory(GB) Cores Description 192 64 GB 28 Dell PowerEdge C6320 server with two 2.4GHz Intel Xeon E5-2680 v4 fourteen-core processors. 75 32 GB 16 Dell PowerEdge R420 server with two 2.1GHz Intel Xeon E5-2450 eight-core processors. 64 64 GB 16 Dell PowerEdge R420 server with two 2.1GHz Intel Xeon E5-2450 eight-core processors 8 512 GB 32 Dell PowerEdge R720 server with four 2.4GHz Intel Xeon E5-4640 eight-core processors. 16 64 GB 28 Dell PowerEdge R730 server with two 2.4GHz Intel Xeon E5-2680 v4 fourteen-core processors and two Nvidia Tesla K80 GPUS ( 4,992 GPU cores/card) .","title":"Compute Node"},{"location":"details/details/#login-node","text":"The Talon 3 login nodes can be access by the domain name: talon3.hpc.unt.edu These login nodes are for users to setup jobs and simple file editing via a Linux Command Line terminal. Users will submit jobs via the SLURM queuing system to be dispatched on the compute nodes. There is NO X11 Forwarding capable on this server.","title":"Login Node"},{"location":"details/details/#visualization-login-nodes","text":"There are five login nodes that have X11 capabilities and are Slurm submission hosts. This host is intended for using the graphical-based software, e.g., gnuplot, matplotlib, and other notebook features in software, such as MATLAB and Mathematica. Users can use these nodes for any post-processing tasks, but any compute-intensive tasks MUST be submitted through the Slurm queuing system. Also, X11 graphical sessions require large bandwidth to work effectively, and you may experience lag while on a home DSL or Cable ISP. X11 forwarding will need to be enabled with the SSH client that you are using. Alternately, you can access a specific visualization node by entering one of the following domain names: vis.acs.unt.edu vis-01.acs.unt.edu vis-02.acs.unt.edu vis-03.acs.unt.edu vis-04.acs.unt.edu","title":"Visualization Login Nodes"},{"location":"details/details/#gpu-node","text":"There are 16 GPU Nodes on Talon. Each one contains 2 Nvidia Tesla K80 GPUs. Quanity Memory(GB) Cores Description 16 64 GB 28 Dell PowerEdge R730 server with two 2.4GHz Intel Xeon E5-2680 v4 fourteen-core processors and two Nvidia Tesla K80 GPUS ( 4,992 GPU cores/card) .","title":"GPU Node"},{"location":"details/details/#job","text":"In an HPC cluster, the users\u2019 tasks to be done on compute nodes are controlled by a batch queuing system. Queuing systems manage job requests (shell scripts generally referred to as jobs) submitted by all users on Talon 3. In other words, to get your computations done by the cluster, you must submit a job request to a specific batch queue. The scheduler will assign your job to a compute node in the order determined by the policy on that queue and the availability of an idle compute node. Currently, Talon 3 resources have several policies in place to help guarantee fair resource utilization from all users.","title":"Job"},{"location":"details/details/#batch-batch-job","text":"Refers to the process of creating a .job file and submit it to HPC using the slurm command sbatch job_name.job where job_name.job is your job file. Here is a brief example of a .job file that runs a python script named test.py : #!/bin/bash #SBATCH -J job_name #SBATCH -o output_job.o%j #SBATCH -e error_job.e%j #SBATCH -p public #SBATCH --qos general #SBATCH -N 1 #SBATCH -n 1 #SBATCH -C c6320 module load python python test.py For more details on how to write a job file please check: Slurm Tutotials","title":"Batch / batch job"},{"location":"details/details/#interractive-session","text":"Interactive job sessions can be used on Talon if you need to compile or test software. An example command of starting an interactive sessions is shown below: $ srun -p public --qos general -C c6320 --pty bash This launches an interactive job session and lanches a bash shell to a compute node. From there, you can execute software and shell commands that would otherwise not be allowed on the Talon login nodes. You can use same Slurm Commands : Check our UNT website Example of requesting 1 node with email notificaiton for CPU: srun -p public --qos general --mail-user = user@unt.edu --mail-type = ALL -N 1 -C c6320 --pty bash Example of requesting 1 node with email notificaiton for GPU: srun -p gpu --mail-user = user@unt.edu --mail-type = ALL -N 1 --pty bash This is very useful because it notifies you when your node has been allocated to you so you can start work! For more details on how to write a job file please check: Slurm Tutotials","title":"Interractive Session"},{"location":"details/details/#modules","text":"Talon 3 uses LMOD to manage your environment. It handles setting up access to the software packages, libraries, and other utilities. To see the list of all available modules, run the command: module avail To see which modules are already loaded: module list To add a module for the current session: module load <module_name> where is the name of the module you wish to load. For example, if you need to use the Intel compilers, you can run module load python/3.6.5 This will load the Python version 3.6.5. Here is a table of current modules that can be found on Talon 3: /cm/shared/modulefiles/science /cm/shared/modulefiles/science /cm/shared/modulefiles/utils JDFTx/1.5.0 R/R-devel PackageEnv/gcc4.8.5_OBLAS0.2.19_OMPI2.1.0 OpenSees/3.0.3 R/3.6.0 (D) PackageEnv/gcc8.1.0_OBLAS0.2.19_OMPI2.1.0 amber/16-cuda-mpi anaconda/5.2 PackageEnv/intel17.0.4_MKL_IMPI_AVX amber/16-gen atlas/3.10.3 PackageEnv/intel17.0.4_MKL_IMPI_AVX2 amber/18-gen (D) bison/3.5.90 PackageEnv/intel18.0.3_gcc4.8.5_MKL_IMPI_AVX ansys/2019r1 boost/1.63.0 PackageEnv/intel18.0.3_gcc4.8.5_MKL_IMPI_AVX2 armadillo/7.950.1 boost/1.71.0 (D) PackageEnv/intel18.0.3_gcc8.1.0_MKL_IMPI_AVX (D) bamtools/2.4.1 bzip2/1.0.6 bazel/0.9.0 cluster-tools/7.3 bazel/0.14.0 (D) cmake/3.8.0 bbmap/38.76 cmake/3.15.2 (D) bcftools/1.4 cmd bcftools/1.4.1 cmgui/7.3 bcftools/1.6 (D) cmsh beast/1.8.4 cuda/101/toolkit/10.1.243 beast/2.4.5 (D) cuda/102/toolkit/10.2.89 bedtools/2.18 cuda/75/blas/7.5.18 bioperl/1.007002 cuda/75/fft/7.5.18 blast/2.6.0 cuda/75/nsight/7.5.18 bowtie2/2.3.1 cuda/75/profiler/7.5.18 bowtie2/2.3.2 (D) cuda/75/toolkit/7.5.18 bsr/bsr cuda/80/blas/8.0.61 bsr/dbsrhf (D) cuda/80/fft/8.0.61 bwa/0.7.17 cuda/80/nsight/8.0.61 caffe/cpu cuda/80/profiler/8.0.61 caffe/gpu (D) cuda/80/toolkit/8.0.61 cairo/1.14.10 cuda/90/blas/9.0.176 cd-hit/4.6.6 cuda/90/fft/9.0.176 cd-hit/4.8.1 (D) cuda/90/nsight/9.0.176 circos/0.69 cuda/90/profiler/9.0.176 clark/1.2.4 cuda/90/toolkit/9.0.176 comsol/5.5 cuda/91/blas/9.1.85 cp2k/6.1 cuda/91/fft/9.1.85 cufflinks/2.2.1 cuda/91/nsight/9.1.85 ddocent/2.2.16 cuda/91/profiler/9.1.85 dirac/2019 cuda/91/toolkit/9.1.85 dlpoly/class-1.10 cudnn/5.1/cuda75 dlpoly/4.08 (D) cudnn/5.1/cuda80 (D) eigen/3.3.3 cudnn/6.0/cuda75 emto/5.8.1 cudnn/6.0/cuda80 (D) espresso/6.0 cudnn/7.0/cuda90 espresso/6.5 (D) cudnn/7.1/cuda90 fastqc/0.11.8 cudnn/7.6.5/cuda90 fastxtoolkit/0.0.13 cudnn/7.6.5/cuda101 fpart/1.0.0 cudnn/7.6.5/cuda102 (D) gamess/08182016 curl/7.54.1 gamess/10302017 (D) ea-utils/2017 gatk/3.80 fftw/2.1.5 gaussian/g09-revA fftw/3.3.6 gaussian/g09-revD (D) fftw/3.3.8 (D) gaussian/g16-RevA.03-ax2 gcc/4.8.5 genenetwork/2 gcc/5.5.0 gistic2/2.0.23 gcc/6.1.0 gnuplot/5.0.6 (D) gcc/6.3.0 grasp/grasp2018 gcc/8.1.0 (L,D) gromacs/5.1.4-fftw git/2.9.3 gromacs/2019.1-cuda glibc/2.23 gromacs/2019.1 (D) gnuplot/5.0.6 gsl/2.4 go/1.10.3 gulp/4.5 gperf/3.1 h2o4pu/0.3.2 gsl/2.6 (D) hadoop/2.8.0-rdma hdf5/1.6.10 hisat/0.1.6 hdf5/1.8.18-gnu hisat/2.0.5 (D) hdf5/1.8.18-intel icu/4c-59 hdf5/1.10.5 (D) jags/4.2.0 hisat/0.1.6 kallisto/0.43.1 hisat/2.0.5 keras/2.1.6 htslib/1.4 keras/2.2.0 (D) htslib/1.6 (D) kraken/0.10 htslib/1.10 kraken/2.0.7 (D) intel/IMPI/17.0.4-AVX lammps/7Aug19 intel/IMPI/17.0.4-AVX2 lammps/11Aug17 intel/IMPI/17.0.4 lammps/17Nov16 (D) intel/IMPI/18.0.3-AVX maps/4.2 intel/IMPI/18.0.3-AVX2 mathematica/10.0 intel/IMPI/18.0.3 (D) mathematica/11.3 (D) intel/compilers/17.0.4-AVX matlab/R2014b intel/compilers/17.0.4-AVX2 matlab/R2016b intel/compilers/18.0.3-AVX (D) matlab/R2018b (D) intel/compilers/18.0.3-AVX2 minimac2/2014.9.15 intel/mkl/17.0.4-AVX mothur/1.43.0 intel/mkl/17.0.4-AVX2 mpiblast/1.6.0 intel/mkl/17.0.4 mplus/8 intel/mkl/18.0.3-AVX mummer/4.0 intel/mkl/18.0.3-AVX2 namd/2.12-cuda intel/mkl/18.0.3 (D) namd/2.12 (D) java/1.8 ncurses/6.0 java/13.0.2 (D) nonmem/7.4.3 jupyter/4.4.0 nwchem/6.8 lapack/3.7.0 nwchem/6.8.1-dev (D) libffi/3.2.1 oases/0.2.8 libtool/2.4.6 oncosnp/1.4 miniconda/2/4.7.12 openfoam/5.0 miniconda/\u00be.7.12 ovito/2.9.0 netcdf/c/4.7.3 paraview/5.3.0 octave/5.2.0 parsync/1.67 openblas/0.2.19 phyml/3.1 opencv/3.4.7 picard/2.10 openmpi/gcc/1.10.6 plink/1.07 openmpi/gcc/2.1.0 plink/1.90 openmpi/gcc/3.1.4 (D) plink/2.00 (D) openmpi/intel/2.1.0-cuda psi4/1.0.0 openmpi/intel/2.1.0 (D) pytorch/gpu-1.1.0 openmpi/pgi/1.10.2 pytorch/gpu-1.2.0 openmpi/pgi/2.1.2 pytorch/gpu-1.4.0 openmpi/pgi/3.1.3 (D) pytorch/gpu-1.5.0 (D) parallel/20200122 qiime2/2020.2 perl/5.24.1 raxml/8.2.10 (D) perl/5.30.2 (D) rohan/1.0 pgi/17.5 sagemath/7.6 pgi/18.10 salmon/1.0 pgi/19.7 (D) samtools/1.4 pssh/2.3.1 samtools/1.4.1 pyMPI/2.4 samtools/1.6 (D) python/2.7.13 seqprep/1.0 python/2.7.14-pyCUDA sklearn/0.18.1 python/3.5.7 soapdenovo/1.03 python/3.6.0 spades/3.10.1 python/3.6.0-2 spades/3.12.0 (D) python/3.6.5 spark/2.3.0 python/3.7.0 spring/spring5 python/3.7.4 (D) sra-tools/2.92 qt/5.14.0 stacks/1.46 raxml/8.2.10 star/2.5.3a readline/7.0 stringtie/1.3.3b singularity/3.6 subread/2.0.0 slurm/16.05.8 (L) tdep/1.1 sparsehash/2.0.2 tecplot/2018R1 szip/1.12b tensorflow/1.2.0 tcl/8.6.7 tensorflow/1.10.1-gpu xalt/2.6 tensorflow/1.10.1 xalt/2.8 (L,D) tensorflow/1.12.3-gpu zlib/1.2.11 tensorflow/2.0 tensorflow/2.1.0-gpu (D) tophat/2.1.1 transformers/transformers trinity/2.4.0 vasp/4.6-vtst vasp/5.4.1-vtst vasp/5.4.1 vasp/5.4.4-vtst (D) vcftools/0.1.17 velvet/1.2.10 visit/2.12.2 vmd/1.9.3 warp/4.5 xoopic/2.70","title":"Module/s"},{"location":"jupyter/jupyter_hub/","text":"Using Jupyter Hub on Talon \u00b6 What is JupyterHub? \u00b6 Jupyter Hub brings the power of notebooks in your browser. It gives users access to computational environments and resources without burdening the users with installation and maintenance tasks. Users - including students, researchers, and data scientists - can get their work done in their own workspaces on shared resources which is managed efficiently by our Talon HPC team. JupyterHub runs on Talon UNT HPC, and makes it possible to serve a pre-configured data science environment to any UNT user. It is customizable and scalable, and is suitable for small and large teams, academic courses, and large-scale infrastructure. Overview \u00b6 First Steup \u00b6 To using Jupyterhub you will first need to follow a few commands on Talon to setup your login 1 2 cd /home/$USER ln -s /work/$USER/.local .local Where is JupyterHub running? \u00b6 JupyterHub is currently running on: http://vis.acs.unt.edu:8000 How can I use JupyterHub on Talon? \u00b6 It is very easy to use it! Here are the steps to follow in order to connect to JupyterHub and start using it: Tip Make sure you are connected to the UNT VPN. Don\u2019t know what/how? Here is the info you need! Open your favorite browser on your local machine/laptop \u00b6 Any browser like Safari, Chrom, or Mozzila will work. Choose JupyterHub Server \u00b6 Currently, the JupyterHub Server is running on our Visualization node which has V100 GPUs. **Go to vis.acs.unt.edu:8000 ** Login Screen \u00b6 UserName: Is your EUID . Password: is your Talon account password. Loading Screen \u00b6 Wait for the server to start\u2026 JupyterHub Launcher \u00b6 This is where you will see all the apps installed on JupyterHub. On the left side it will look different for you. This is where you can navigate through your files. Start Jupyter Notebook \u00b6 In order to start a regular Jupyter Notebook in Python use the Python 3 icon app: Feel free to try out other apps! Good Practice \u00b6 This will ensure you don\u2019t lose any work! Stop server \u00b6 Make sure to stop your server if you don\u2019t need it anymore. File > Hub Control Panel > Stop My Server After stopping the server make sure to log out: on the top right press Logout Log Out \u00b6 This will not stop your server! File > Log Out","title":"Jupyter Hub"},{"location":"jupyter/jupyter_hub/#using-jupyter-hub-on-talon","text":"","title":"Using Jupyter Hub on Talon"},{"location":"jupyter/jupyter_hub/#what-is-jupyterhub","text":"Jupyter Hub brings the power of notebooks in your browser. It gives users access to computational environments and resources without burdening the users with installation and maintenance tasks. Users - including students, researchers, and data scientists - can get their work done in their own workspaces on shared resources which is managed efficiently by our Talon HPC team. JupyterHub runs on Talon UNT HPC, and makes it possible to serve a pre-configured data science environment to any UNT user. It is customizable and scalable, and is suitable for small and large teams, academic courses, and large-scale infrastructure.","title":"What is JupyterHub?"},{"location":"jupyter/jupyter_hub/#overview","text":"","title":"Overview"},{"location":"jupyter/jupyter_hub/#first-steup","text":"To using Jupyterhub you will first need to follow a few commands on Talon to setup your login 1 2 cd /home/$USER ln -s /work/$USER/.local .local","title":"First Steup"},{"location":"jupyter/jupyter_hub/#where-is-jupyterhub-running","text":"JupyterHub is currently running on: http://vis.acs.unt.edu:8000","title":"Where is JupyterHub running?"},{"location":"jupyter/jupyter_hub/#how-can-i-use-jupyterhub-on-talon","text":"It is very easy to use it! Here are the steps to follow in order to connect to JupyterHub and start using it: Tip Make sure you are connected to the UNT VPN. Don\u2019t know what/how? Here is the info you need!","title":"How can I use JupyterHub on Talon?"},{"location":"jupyter/jupyter_hub/#open-your-favorite-browser-on-your-local-machinelaptop","text":"Any browser like Safari, Chrom, or Mozzila will work.","title":"Open your favorite browser on your local machine/laptop"},{"location":"jupyter/jupyter_hub/#choose-jupyterhub-server","text":"Currently, the JupyterHub Server is running on our Visualization node which has V100 GPUs. **Go to vis.acs.unt.edu:8000 **","title":"Choose JupyterHub Server"},{"location":"jupyter/jupyter_hub/#login-screen","text":"UserName: Is your EUID . Password: is your Talon account password.","title":"Login Screen"},{"location":"jupyter/jupyter_hub/#loading-screen","text":"Wait for the server to start\u2026","title":"Loading Screen"},{"location":"jupyter/jupyter_hub/#jupyterhub-launcher","text":"This is where you will see all the apps installed on JupyterHub. On the left side it will look different for you. This is where you can navigate through your files.","title":"JupyterHub Launcher"},{"location":"jupyter/jupyter_hub/#start-jupyter-notebook","text":"In order to start a regular Jupyter Notebook in Python use the Python 3 icon app: Feel free to try out other apps!","title":"Start Jupyter Notebook"},{"location":"jupyter/jupyter_hub/#good-practice","text":"This will ensure you don\u2019t lose any work!","title":"Good Practice"},{"location":"jupyter/jupyter_hub/#stop-server","text":"Make sure to stop your server if you don\u2019t need it anymore. File > Hub Control Panel > Stop My Server After stopping the server make sure to log out: on the top right press Logout","title":"Stop server"},{"location":"jupyter/jupyter_hub/#log-out","text":"This will not stop your server! File > Log Out","title":"Log Out"},{"location":"jupyter/jupyter_notebook/","text":"Jupyter Notebook on Talon HPC \u00b6 These are all the instructions on how to start a YOUR OWN Jupyter Notebook server on Talon and how to access it from your computer. There are two ways you can run Jupyter Notebook, either as an Interactive node job This is for SMALL TEST notebooks that are ran on the visulization login node !!! warning Interactive notebooks maybe terminated if using too much computing resources, taking too long, or taking too much memory. Compute node job This is for more larger, intensive jobs You will run a notebook on a compute node via the SLURM queuing job This is required for using GPU functions because you need GPU compute nodes First time ONLY \u00b6 Execute this step to setup password for your Jupyter Notebook. Login to a Visualization login nodes. Use ONE of the following: $ ssh YOUR_EUID@vis.acs.unt.edu Load the appropriate module: $ module load python/3.6.5 Type this in your terminal: $ jupyter notebook password Then enter a password. This will be the password necessary to access Jupyter Notebook. THIS IS NOT YOUR EUID PASSWORD. You will have to type twice, for: $ Enter password: And $ Verify password: After you entered the password twice you should see a message: [ NotebookPasswordApp ] Wrote hashed password to /home/YOUR_EUID/.jupyter/jupyter_notebook_config.json This creates a configuration file jupyter_notebook_config.json related to your password. Now you can run Jupyter Notebook on Talon in a few ways: Run on an interactive node \u00b6 Login to a Visualization login nodes. Use ONE of the following: 1 $ ssh YOUR_EUID@vis.acs.unt.edu Load the appropriate module: 1 $ module load python / 3.6 . 5 Launch Jupyter Notebook: Enter the following command in your terminal: $ jupyter notebook This will spawn a Jupyter Notebook terminal window that looks like this: As long as you want to use Jupyter Notebook you need to keep this terminal open. If you close this terminal you will lose your Jupyter Notebook session and any unsaved information! Keep track of the port that the notebook is running on. in the window you see $ REFRESH ( 1 sec ) : http://localhost:8891/tree 8891 is the port number in this case. In your case it can be a different number! That will be YOUR_PORT_NUMBER Forward Jupyter Notebook to your local machine browser Now you need to open a new terminal window and use the following command: 1 $ ssh -L YOUR_PORT_NUMBER:localhost:YOUR_PORT_NUMBER YOUR_EUDI@vis.acs.unt.edu In this example the port number is 8891, so I will use: 1 ssh -L 8891:localhost:8891 EUID@vis.acs.unt.edu You will replace this number with the one generated in your Jupyter Notebook terminal window. This will forward the port of the Jupyter Notebook running on talon to your local machine. Double check\u2026 Now you should have 2 terminals running: One terminal with the Jupyter Notebook terminal window like in 1.3. Launch Jupyter Notebook Another terminal that you used to login with like in 1.4 Forward Jupyter Notebook to your local machine browser : $ ssh -L YOUR_PORT_NUMBER:localhost:YOUR_PORT_NUMBER YOUR_EUDI@vis.acs.unt.edu These 2 terminal are the ones keeping your Jupyter Notebook alive and running on your local machine! Access your Jupyter Notebook: Now to access the Jupyter Notebook, open any browser (Chrome,Mozilla etc) and type: 1 http://localhost:YOUR_PORT_NUMBER In my case, my port number is 8891 so I will have to use: 1 http://localhost:8891 You will see a webpage like this: This is where you will enter your password that you created when you used 1 $ jupyter notebook password Now you are inside your Jupyer Notebook on your local machine browser that runs on Talon! Cool! This is how mine looks like after login: I only have bin folder in this image. Yours will be different, you will see all your directories from Talon. Run as a job file \u00b6 Login to Talon: 1 $ ssh YOUR_EUID@talon3.hpc.unt.edu Create and configure your .job file: Now you will need to create a .job file just like a batch submisison job (Check this out if not familiar with job submissions on Talon) Your .job file should look like this: #!/bin/bash #SBATCH -p preproduction #SBATCH --qos general #SBATCH -N 1 #SBATCH --ntasks-per-node=16 module load python/3.6.5 unset XDG_RUNTIME_DIR jupyter notebook --no-browser --ip = 0 .0.0.0 The first 6 lines are configurable to anything you want. See Talon 3 job submission for more details. You can add/remove resources or functionality to it just like with any job file. The part that is very important for Jupyter Notebook in the job file is: 1 2 3 module load python / 3.6 . 5 unset XDG_RUNTIME_DIR jupyter notebook -- no - browser -- ip = 0.0 . 0.0 Once you have the job file ready, you can launch it just like any job file on talon 1 $ sbatch YOUR_JOB_FILE.job This will launch your job supporting Jupyter Notebooks. We need to find the compute node where the notebooks is running. For this we will use the output file generated by the job file (For example: job_JOB_ID.out) should look like: 1 2 3 4 [I 12:33:10.043 NotebookApp] Serving notebooks from local directory: /home/gm0234 [I 12:33:10.043 NotebookApp] The Jupyter Notebook is running at: [I 12:33:10.043 NotebookApp] http://(YOUR_NODE_ADDRESS or 127.0.0.1):YOUR_PORT_NUMBER/ [I 12:33:10.043 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation). As an example: 1 2 3 4 [I 12:33:10.043 NotebookApp] Serving notebooks from local directory: /home/gm0234 [I 12:33:10.043 NotebookApp] The Jupyter Notebook is running at: [I 12:33:10.043 NotebookApp] http://(c32-9-29 or 127.0.0.1):8888/ [I 12:33:10.043 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation). The part we care about is: 1 [I 12:33:10.043 NotebookApp] http://(c32-9-29 or 127.0.0.1):8888/ More exactly: 1 (c32-9-29 or 127.0.0.1):8888/ From this we know the YOUR_NODE_ADDRESS : 1 c32-9-29 And YOUR_PORT_NUMBER : 1 8888 Having these two (YOUR_NODE_ADDRESS and YOUR_PORT_NUMBER) we can open a new terminal and login to forward that address to your local machine: 1 $ ssh -L YOUR_PORT_NUMBER:YOUR_NODE_ADDRESS:YOUR_PORT_NUMBER YOUR_EUID@vis.acs.unt.edu For this example: 1 $ ssh -L 8888 :c32-9-29:8888 YOUR_EUID@vis.acs.unt.edu Access your Jupyter Notebook: Now to access the Jupyter Notebook, open any browser (Chrome,Mozilla etc) and type: 1 http://localhost:YOUR_PORT_NUMBER In my case, my port number is 8888 so I will have to use: 1 http://localhost:8888 You will see a webpage like this: This is where you will enter your password that you created when you used 1 $ jupyter notebook password Now you are inside your Jupyer Notebook on your local machine browser that runs on Talon! Cool! This is how mine looks like after login: I only have bin folder in this image. Yours will be different, you will see all your directories from Talon. Notebooks on GPUs (use GPUs on your notebook) \u00b6 Follow same steps as Run as a job file but use a different .job file: #!/bin/bash #SBATCH -J jupyter #SBATCH -o job_%j.out #SBATCH -p gpu #SBATCH --qos general #SBATCH -N 1 #SBATCH --gres=gpu:4 #SBATCH --mail-type=begin #SBATCH --mail-user=username@my.unt.edu module load pytorch/1.0.1 unset XDG_RUNTIME_DIR jupyter notebook --no-browser --ip = 0 .0.0.0 The first 6 lines are configurable to anything you want. See Talon 3 job submission for more details. You can add/remove resources or functionality to it just like with any job file.","title":"Jupyter Notebook"},{"location":"jupyter/jupyter_notebook/#jupyter-notebook-on-talon-hpc","text":"These are all the instructions on how to start a YOUR OWN Jupyter Notebook server on Talon and how to access it from your computer. There are two ways you can run Jupyter Notebook, either as an Interactive node job This is for SMALL TEST notebooks that are ran on the visulization login node !!! warning Interactive notebooks maybe terminated if using too much computing resources, taking too long, or taking too much memory. Compute node job This is for more larger, intensive jobs You will run a notebook on a compute node via the SLURM queuing job This is required for using GPU functions because you need GPU compute nodes","title":"Jupyter Notebook on Talon HPC"},{"location":"jupyter/jupyter_notebook/#first-time-only","text":"Execute this step to setup password for your Jupyter Notebook. Login to a Visualization login nodes. Use ONE of the following: $ ssh YOUR_EUID@vis.acs.unt.edu Load the appropriate module: $ module load python/3.6.5 Type this in your terminal: $ jupyter notebook password Then enter a password. This will be the password necessary to access Jupyter Notebook. THIS IS NOT YOUR EUID PASSWORD. You will have to type twice, for: $ Enter password: And $ Verify password: After you entered the password twice you should see a message: [ NotebookPasswordApp ] Wrote hashed password to /home/YOUR_EUID/.jupyter/jupyter_notebook_config.json This creates a configuration file jupyter_notebook_config.json related to your password. Now you can run Jupyter Notebook on Talon in a few ways:","title":"First time ONLY"},{"location":"jupyter/jupyter_notebook/#run-on-an-interactive-node","text":"Login to a Visualization login nodes. Use ONE of the following: 1 $ ssh YOUR_EUID@vis.acs.unt.edu Load the appropriate module: 1 $ module load python / 3.6 . 5 Launch Jupyter Notebook: Enter the following command in your terminal: $ jupyter notebook This will spawn a Jupyter Notebook terminal window that looks like this: As long as you want to use Jupyter Notebook you need to keep this terminal open. If you close this terminal you will lose your Jupyter Notebook session and any unsaved information! Keep track of the port that the notebook is running on. in the window you see $ REFRESH ( 1 sec ) : http://localhost:8891/tree 8891 is the port number in this case. In your case it can be a different number! That will be YOUR_PORT_NUMBER Forward Jupyter Notebook to your local machine browser Now you need to open a new terminal window and use the following command: 1 $ ssh -L YOUR_PORT_NUMBER:localhost:YOUR_PORT_NUMBER YOUR_EUDI@vis.acs.unt.edu In this example the port number is 8891, so I will use: 1 ssh -L 8891:localhost:8891 EUID@vis.acs.unt.edu You will replace this number with the one generated in your Jupyter Notebook terminal window. This will forward the port of the Jupyter Notebook running on talon to your local machine. Double check\u2026 Now you should have 2 terminals running: One terminal with the Jupyter Notebook terminal window like in 1.3. Launch Jupyter Notebook Another terminal that you used to login with like in 1.4 Forward Jupyter Notebook to your local machine browser : $ ssh -L YOUR_PORT_NUMBER:localhost:YOUR_PORT_NUMBER YOUR_EUDI@vis.acs.unt.edu These 2 terminal are the ones keeping your Jupyter Notebook alive and running on your local machine! Access your Jupyter Notebook: Now to access the Jupyter Notebook, open any browser (Chrome,Mozilla etc) and type: 1 http://localhost:YOUR_PORT_NUMBER In my case, my port number is 8891 so I will have to use: 1 http://localhost:8891 You will see a webpage like this: This is where you will enter your password that you created when you used 1 $ jupyter notebook password Now you are inside your Jupyer Notebook on your local machine browser that runs on Talon! Cool! This is how mine looks like after login: I only have bin folder in this image. Yours will be different, you will see all your directories from Talon.","title":"Run on an interactive node"},{"location":"jupyter/jupyter_notebook/#run-as-a-job-file","text":"Login to Talon: 1 $ ssh YOUR_EUID@talon3.hpc.unt.edu Create and configure your .job file: Now you will need to create a .job file just like a batch submisison job (Check this out if not familiar with job submissions on Talon) Your .job file should look like this: #!/bin/bash #SBATCH -p preproduction #SBATCH --qos general #SBATCH -N 1 #SBATCH --ntasks-per-node=16 module load python/3.6.5 unset XDG_RUNTIME_DIR jupyter notebook --no-browser --ip = 0 .0.0.0 The first 6 lines are configurable to anything you want. See Talon 3 job submission for more details. You can add/remove resources or functionality to it just like with any job file. The part that is very important for Jupyter Notebook in the job file is: 1 2 3 module load python / 3.6 . 5 unset XDG_RUNTIME_DIR jupyter notebook -- no - browser -- ip = 0.0 . 0.0 Once you have the job file ready, you can launch it just like any job file on talon 1 $ sbatch YOUR_JOB_FILE.job This will launch your job supporting Jupyter Notebooks. We need to find the compute node where the notebooks is running. For this we will use the output file generated by the job file (For example: job_JOB_ID.out) should look like: 1 2 3 4 [I 12:33:10.043 NotebookApp] Serving notebooks from local directory: /home/gm0234 [I 12:33:10.043 NotebookApp] The Jupyter Notebook is running at: [I 12:33:10.043 NotebookApp] http://(YOUR_NODE_ADDRESS or 127.0.0.1):YOUR_PORT_NUMBER/ [I 12:33:10.043 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation). As an example: 1 2 3 4 [I 12:33:10.043 NotebookApp] Serving notebooks from local directory: /home/gm0234 [I 12:33:10.043 NotebookApp] The Jupyter Notebook is running at: [I 12:33:10.043 NotebookApp] http://(c32-9-29 or 127.0.0.1):8888/ [I 12:33:10.043 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation). The part we care about is: 1 [I 12:33:10.043 NotebookApp] http://(c32-9-29 or 127.0.0.1):8888/ More exactly: 1 (c32-9-29 or 127.0.0.1):8888/ From this we know the YOUR_NODE_ADDRESS : 1 c32-9-29 And YOUR_PORT_NUMBER : 1 8888 Having these two (YOUR_NODE_ADDRESS and YOUR_PORT_NUMBER) we can open a new terminal and login to forward that address to your local machine: 1 $ ssh -L YOUR_PORT_NUMBER:YOUR_NODE_ADDRESS:YOUR_PORT_NUMBER YOUR_EUID@vis.acs.unt.edu For this example: 1 $ ssh -L 8888 :c32-9-29:8888 YOUR_EUID@vis.acs.unt.edu Access your Jupyter Notebook: Now to access the Jupyter Notebook, open any browser (Chrome,Mozilla etc) and type: 1 http://localhost:YOUR_PORT_NUMBER In my case, my port number is 8888 so I will have to use: 1 http://localhost:8888 You will see a webpage like this: This is where you will enter your password that you created when you used 1 $ jupyter notebook password Now you are inside your Jupyer Notebook on your local machine browser that runs on Talon! Cool! This is how mine looks like after login: I only have bin folder in this image. Yours will be different, you will see all your directories from Talon.","title":"Run as a job file"},{"location":"jupyter/jupyter_notebook/#notebooks-on-gpus-use-gpus-on-your-notebook","text":"Follow same steps as Run as a job file but use a different .job file: #!/bin/bash #SBATCH -J jupyter #SBATCH -o job_%j.out #SBATCH -p gpu #SBATCH --qos general #SBATCH -N 1 #SBATCH --gres=gpu:4 #SBATCH --mail-type=begin #SBATCH --mail-user=username@my.unt.edu module load pytorch/1.0.1 unset XDG_RUNTIME_DIR jupyter notebook --no-browser --ip = 0 .0.0.0 The first 6 lines are configurable to anything you want. See Talon 3 job submission for more details. You can add/remove resources or functionality to it just like with any job file.","title":"Notebooks on GPUs (use GPUs on your notebook)"},{"location":"jupyter_hub/jupyter_hub/","text":"Using Jupyter Hub on Talon \u00b6 What is JupyterHub? \u00b6 Jupyter Hub brings the power of notebooks in your browser. It gives users access to computational environments and resources without burdening the users with installation and maintenance tasks. Users - including students, researchers, and data scientists - can get their work done in their own workspaces on shared resources which is managed efficiently by our Talon HPC team. JupyterHub runs on Talon UNT HPC, and makes it possible to serve a pre-configured data science environment to any UNT user. It is customizable and scalable, and is suitable for small and large teams, academic courses, and large-scale infrastructure. Overview \u00b6 Where is JupyterHub running? \u00b6 JupyterHub is currently running on: vis.acs.unt.edu vis-04.acs.unt.edu How can I use JupyterHub on Talon? \u00b6 It is very easy to use it! Here are the steps to follow in order to connect to JupyterHub and start using it: Make sure you are connected to the UNT VPN. \u00b6 Don\u2019t know what/how? Here and here is the info you need! Open your favorite browser on your local machine/laptop \u00b6 Any browser like Safari, Chrom, Mozzila will work. Choose JupyterHub Server \u00b6 vis.acs.unt.edu \u00b6 This is a more recent Visualization Login Nodes that has V100 GPU. Go to vis.acs.unt.edu:8000 vis-04.acs.unt.edu \u00b6 This is a relative older Visualization Login Nodes that has no GPU support. Go to vis-04.acs.unt.edu:8000 Login Screen \u00b6 UserName: Is your EUID . Password: is your Talon account password. Loading Screen \u00b6 Wait for the server to start\u2026 JupyterHub Launcher \u00b6 This is where you will see all the apps installed on JupyterHub. On the left side it will look different for you. This is where you can navigate through your files. Start Jupyter Notebook \u00b6 In order to start a regular Jupyter Notebook in Python use the Python 3 icon app: Feel free to try out other apps! Good Practice \u00b6 This will ensure you don\u2019t lose any work! Stop server \u00b6 Make sure to stop your server if you don\u2019t need it anymore. File > Hub Control Panel > Stop My Server After stopping the server make sure to log out: on the top right press Logout Log Out \u00b6 This will not stop your server! File > Log Out","title":"Using Jupyter Hub on Talon"},{"location":"jupyter_hub/jupyter_hub/#using-jupyter-hub-on-talon","text":"","title":"Using Jupyter Hub on Talon"},{"location":"jupyter_hub/jupyter_hub/#what-is-jupyterhub","text":"Jupyter Hub brings the power of notebooks in your browser. It gives users access to computational environments and resources without burdening the users with installation and maintenance tasks. Users - including students, researchers, and data scientists - can get their work done in their own workspaces on shared resources which is managed efficiently by our Talon HPC team. JupyterHub runs on Talon UNT HPC, and makes it possible to serve a pre-configured data science environment to any UNT user. It is customizable and scalable, and is suitable for small and large teams, academic courses, and large-scale infrastructure.","title":"What is JupyterHub?"},{"location":"jupyter_hub/jupyter_hub/#overview","text":"","title":"Overview"},{"location":"jupyter_hub/jupyter_hub/#where-is-jupyterhub-running","text":"JupyterHub is currently running on: vis.acs.unt.edu vis-04.acs.unt.edu","title":"Where is JupyterHub running?"},{"location":"jupyter_hub/jupyter_hub/#how-can-i-use-jupyterhub-on-talon","text":"It is very easy to use it! Here are the steps to follow in order to connect to JupyterHub and start using it:","title":"How can I use JupyterHub on Talon?"},{"location":"jupyter_hub/jupyter_hub/#make-sure-you-are-connected-to-the-unt-vpn","text":"Don\u2019t know what/how? Here and here is the info you need!","title":"Make sure you are connected to the UNT VPN."},{"location":"jupyter_hub/jupyter_hub/#open-your-favorite-browser-on-your-local-machinelaptop","text":"Any browser like Safari, Chrom, Mozzila will work.","title":"Open your favorite browser on your local machine/laptop"},{"location":"jupyter_hub/jupyter_hub/#choose-jupyterhub-server","text":"","title":"Choose  JupyterHub Server"},{"location":"jupyter_hub/jupyter_hub/#visacsuntedu","text":"This is a more recent Visualization Login Nodes that has V100 GPU. Go to vis.acs.unt.edu:8000","title":"vis.acs.unt.edu"},{"location":"jupyter_hub/jupyter_hub/#vis-04acsuntedu","text":"This is a relative older Visualization Login Nodes that has no GPU support. Go to vis-04.acs.unt.edu:8000","title":"vis-04.acs.unt.edu"},{"location":"jupyter_hub/jupyter_hub/#login-screen","text":"UserName: Is your EUID . Password: is your Talon account password.","title":"Login Screen"},{"location":"jupyter_hub/jupyter_hub/#loading-screen","text":"Wait for the server to start\u2026","title":"Loading Screen"},{"location":"jupyter_hub/jupyter_hub/#jupyterhub-launcher","text":"This is where you will see all the apps installed on JupyterHub. On the left side it will look different for you. This is where you can navigate through your files.","title":"JupyterHub Launcher"},{"location":"jupyter_hub/jupyter_hub/#start-jupyter-notebook","text":"In order to start a regular Jupyter Notebook in Python use the Python 3 icon app: Feel free to try out other apps!","title":"Start Jupyter Notebook"},{"location":"jupyter_hub/jupyter_hub/#good-practice","text":"This will ensure you don\u2019t lose any work!","title":"Good Practice"},{"location":"jupyter_hub/jupyter_hub/#stop-server","text":"Make sure to stop your server if you don\u2019t need it anymore. File > Hub Control Panel > Stop My Server After stopping the server make sure to log out: on the top right press Logout","title":"Stop server"},{"location":"jupyter_hub/jupyter_hub/#log-out","text":"This will not stop your server! File > Log Out","title":"Log Out"},{"location":"other/ntscmeeting/","text":"NTSC Meetings \u00b6 October 2020 TUG Meeting \u00b6 Topic: \u201cThe limits of quantum circuit simulation with low precision arithmetic using Talon\u201d by Santiago Betelu, visiting professor of mathematics, UNT College of Science, and chief scientist at Data Vortex When: Oct. 21, 2020 1 pm Encouraged by feedback from the Talon User Group meetings on Sept. 21 and 23, the TUG Engagement and Outreach Series has been developed. The purpose of this monthly event is to listen to Talon users, explore opportunities in furthering Talon service effectiveness and enable, foster, and develop new research and training opportunities in scientific computing and analytics. Dr. Betelu\u2019s abstract: This is an investigation of the limits of quantum circuit simulation with Schrodinger\u2019s formulation and low precision arithmetic. The goal is to estimate how much memory can be saved in simulations that involve random, maximally entangled quantum states. An arithmetic polar representation of B bits is defined for each quantum amplitude and a normalization procedure is developed to minimize rounding errors. Then a model is developed to quantify the cumulative errors on a circuit of Q qubits and G gates. Depending on which regime the circuit operates, the model yields explicit expressions for the maximum number of effective gates that can be simulated before rounding errors dominate the computation. The results are illustrated with random circuits and the quantum Fourier transform. Click here to access the slides from Dr. Betelu\u2019s talk Fall 2020 Talon User Group Meeting \u00b6 Virtual Meeting with the North Texas Scientific Computing Staff that took place on Sept. 21 and 23. Click here to access the slides from the meeting!","title":"NTSC Meetings"},{"location":"other/ntscmeeting/#ntsc-meetings","text":"","title":"NTSC Meetings"},{"location":"other/ntscmeeting/#october-2020-tug-meeting","text":"Topic: \u201cThe limits of quantum circuit simulation with low precision arithmetic using Talon\u201d by Santiago Betelu, visiting professor of mathematics, UNT College of Science, and chief scientist at Data Vortex When: Oct. 21, 2020 1 pm Encouraged by feedback from the Talon User Group meetings on Sept. 21 and 23, the TUG Engagement and Outreach Series has been developed. The purpose of this monthly event is to listen to Talon users, explore opportunities in furthering Talon service effectiveness and enable, foster, and develop new research and training opportunities in scientific computing and analytics. Dr. Betelu\u2019s abstract: This is an investigation of the limits of quantum circuit simulation with Schrodinger\u2019s formulation and low precision arithmetic. The goal is to estimate how much memory can be saved in simulations that involve random, maximally entangled quantum states. An arithmetic polar representation of B bits is defined for each quantum amplitude and a normalization procedure is developed to minimize rounding errors. Then a model is developed to quantify the cumulative errors on a circuit of Q qubits and G gates. Depending on which regime the circuit operates, the model yields explicit expressions for the maximum number of effective gates that can be simulated before rounding errors dominate the computation. The results are illustrated with random circuits and the quantum Fourier transform. Click here to access the slides from Dr. Betelu\u2019s talk","title":"October 2020 TUG Meeting"},{"location":"other/ntscmeeting/#fall-2020-talon-user-group-meeting","text":"Virtual Meeting with the North Texas Scientific Computing Staff that took place on Sept. 21 and 23. Click here to access the slides from the meeting!","title":"Fall 2020 Talon User Group Meeting"},{"location":"overview/cite/","text":"Citing Talon3 \u00b6 Please acknowledge the support of the High-Performance Computing services in all publications with the following statement. \u201cComputational resources were provided by the North Texas Scientific Computing, a division under the of office of the CIO for UNT and UNT System\u201d","title":"Citing Talon Work"},{"location":"overview/cite/#citing-talon3","text":"Please acknowledge the support of the High-Performance Computing services in all publications with the following statement. \u201cComputational resources were provided by the North Texas Scientific Computing, a division under the of office of the CIO for UNT and UNT System\u201d","title":"Citing Talon3"},{"location":"overview/filesystem/","text":"File System Overview \u00b6 Talon 3 has a variety of file systems. It is crucial that you understand the proper use of these file systems for best performance and stewardship of computational resources. Misuse of the file systems could mean the termination of your jobs or loss of your data. Summary \u00b6 Name Path Type Quota Other Information HOME /home/$USER ext4/NFS 20GB per user Backed up Daily SCRATCH /storage/scratch2/$USER Lustre 25TB per allocation No Backup. No data purged WORK /work/$USER Lustre 2TB per user No Backup. Subject to purge at high usage","title":"File System"},{"location":"overview/filesystem/#file-system-overview","text":"Talon 3 has a variety of file systems. It is crucial that you understand the proper use of these file systems for best performance and stewardship of computational resources. Misuse of the file systems could mean the termination of your jobs or loss of your data.","title":"File System Overview"},{"location":"overview/filesystem/#summary","text":"Name Path Type Quota Other Information HOME /home/$USER ext4/NFS 20GB per user Backed up Daily SCRATCH /storage/scratch2/$USER Lustre 25TB per allocation No Backup. No data purged WORK /work/$USER Lustre 2TB per user No Backup. Subject to purge at high usage","title":"Summary"},{"location":"overview/nodes/","text":"Talon3 Compute Nodes \u00b6 Talon3 is a heterogeneous cluster with different compute node types. Login Node \u00b6 The Talon 3 login nodes can be access by the domain name: talon3.hpc.unt.edu These login nodes are for users to setup jobs and simple file editing via a Linux Command Line terminal. Users will submit jobs via the SLURM queuing system to be dispatched on the compute nodes. There is NO X11 Forwarding capable on this server. Compute Nodes \u00b6 The Talon 3 compute nodes are Dell PowerEdge servers where the users will run their software and other computationally intensive applications. Users do NOT directly login to these nodes . Users run their calculations on these nodes by submitting them to the Slurm queuing system. Type Quanity Memory(GB) Cores Description C6320 192 64 GB 28 two 2.4GHz Intel Xeon E5-2680 v4 fourteen-core processors R420 75 32 GB 16 two 2.1GHz Intel Xeon E5-2450 eight-core processors R420 64 64 GB 16 two 2.1GHz Intel Xeon E5-2450 eight-core processors R720 8 512 GB 32 four 2.4GHz Intel Xeon E5-4640 eight-core processors The R420 nodes are available to ALL Talon3 allocations. This nodes can be access via the preproducation parition in SLURM. #SBATCH -p preproducation The R720 nodes are the big memory nodes that are available to ALL Talon3 allocations that can be access with the bigmem parition. #SBATCH -p bigmem The C6320 access is restricted to Talon3 allocations that recorded 300,000 CPU hours or more during the current or previous fiscal year or have active external funding. These nodes can be access via the producation parition in SLURM. #SBATCH -p production GPU Node \u00b6 There are 16 GPU Nodes on Talon. Each one contains 2 Nvidia Tesla K80 GPUs. Type Quanity Memory(GB) CPU Cores GPU Description R730 16 64 GB 28 4 GPU cards two 2.4GHz Intel Xeon E5-2680 v4 fourteen-core processors/ two Nvidia Tesla K80 GPUS (4,992 GPU cores/card) The R730 GPU nodes are availabe to ALL Talon3 allocation. #SBATCH -p gpu Visualization Login Nodes \u00b6 There are nodes that have X11 capabilities and are Slurm submission hosts. This host is intended for using the graphical-based software, e.g., gnuplot, matplotlib, and other notebook features in software, such as MATLAB and Mathematica. Users can use these nodes for any post-processing tasks, but any compute-intensive tasks MUST be submitted through the Slurm queuing system. These visualization nodes provide the same funcationally as the login nodes and more. Jupyter Notebooks and Rstudio can be access to these nodes. Also, X11 graphical sessions require large bandwidth to work effectively, and you may experience lag while on a home DSL or Cable ISP. X11 forwarding will need to be enabled with the SSH client that you are using. Alternately, you can access a specific visualization node by entering one of the following domain names: vis.acs.unt.edu vis-06.acs.unt.edu vis-07.acs.unt.edu","title":"Compute Nodes"},{"location":"overview/nodes/#talon3-compute-nodes","text":"Talon3 is a heterogeneous cluster with different compute node types.","title":"Talon3 Compute Nodes"},{"location":"overview/nodes/#login-node","text":"The Talon 3 login nodes can be access by the domain name: talon3.hpc.unt.edu These login nodes are for users to setup jobs and simple file editing via a Linux Command Line terminal. Users will submit jobs via the SLURM queuing system to be dispatched on the compute nodes. There is NO X11 Forwarding capable on this server.","title":"Login Node"},{"location":"overview/nodes/#compute-nodes","text":"The Talon 3 compute nodes are Dell PowerEdge servers where the users will run their software and other computationally intensive applications. Users do NOT directly login to these nodes . Users run their calculations on these nodes by submitting them to the Slurm queuing system. Type Quanity Memory(GB) Cores Description C6320 192 64 GB 28 two 2.4GHz Intel Xeon E5-2680 v4 fourteen-core processors R420 75 32 GB 16 two 2.1GHz Intel Xeon E5-2450 eight-core processors R420 64 64 GB 16 two 2.1GHz Intel Xeon E5-2450 eight-core processors R720 8 512 GB 32 four 2.4GHz Intel Xeon E5-4640 eight-core processors The R420 nodes are available to ALL Talon3 allocations. This nodes can be access via the preproducation parition in SLURM. #SBATCH -p preproducation The R720 nodes are the big memory nodes that are available to ALL Talon3 allocations that can be access with the bigmem parition. #SBATCH -p bigmem The C6320 access is restricted to Talon3 allocations that recorded 300,000 CPU hours or more during the current or previous fiscal year or have active external funding. These nodes can be access via the producation parition in SLURM. #SBATCH -p production","title":"Compute Nodes"},{"location":"overview/nodes/#gpu-node","text":"There are 16 GPU Nodes on Talon. Each one contains 2 Nvidia Tesla K80 GPUs. Type Quanity Memory(GB) CPU Cores GPU Description R730 16 64 GB 28 4 GPU cards two 2.4GHz Intel Xeon E5-2680 v4 fourteen-core processors/ two Nvidia Tesla K80 GPUS (4,992 GPU cores/card) The R730 GPU nodes are availabe to ALL Talon3 allocation. #SBATCH -p gpu","title":"GPU Node"},{"location":"overview/nodes/#visualization-login-nodes","text":"There are nodes that have X11 capabilities and are Slurm submission hosts. This host is intended for using the graphical-based software, e.g., gnuplot, matplotlib, and other notebook features in software, such as MATLAB and Mathematica. Users can use these nodes for any post-processing tasks, but any compute-intensive tasks MUST be submitted through the Slurm queuing system. These visualization nodes provide the same funcationally as the login nodes and more. Jupyter Notebooks and Rstudio can be access to these nodes. Also, X11 graphical sessions require large bandwidth to work effectively, and you may experience lag while on a home DSL or Cable ISP. X11 forwarding will need to be enabled with the SSH client that you are using. Alternately, you can access a specific visualization node by entering one of the following domain names: vis.acs.unt.edu vis-06.acs.unt.edu vis-07.acs.unt.edu","title":"Visualization Login Nodes"},{"location":"pycharm/pycharm/","text":"How to setup PyCharm for Talon \u00b6 Apply for professional account \u00b6 In order to register for a professional account use please go to: https://www.jetbrains.com/student/ Apply: Download PyCharm Professional \u00b6 Go to: https://www.jetbrains.com/pycharm/ Press Download: Choose Professional version [in this example I have use Linux. For Windows or Mac will be slightly different]: Install PyCharm: \u00b6 Follow normal installation instruction Fisrt time setup: Skip Remaining and Set Defaults Welcome to PyCharm Go to Settings Now click on Project Interpreter Click on right wheel Add\u2026 SSH Interpreter New server configuration: enter Talon Vis node address: vis-01.acs.unt.edu Asks for Connecting To Remote Host: Accept [It might ask for your password instead] It mgiht also ask you to enter password of your JetBrain account. You created this account earlier. [Make sure to enter password for that account] Default remote interpreter: Brows to add a new interpreter: Enter path of Talon Python interpreter: Set path interpreter: Path Interpreter should look like this now: Project Interpreter should look like this now: Need to wait for a while to finish: Test with creating a new project \u00b6 Create New Project Select Pure Python Choose preffered name of project. In this case is test Now we need to set interpreter: Click Project Interpreter > Existing Interpreter: In Remote project location make sure add the path on Talon where you want your project to be! Wait for some time: If no folder is created wiht project name in Talon, PyCharm will create it for you. Create Python file: Name your python test file: File uploaded to Talon when it\u2019s created: Write a print(\u201cHello world!\u201d) in python file. When save the file it will automatichally save it to Talon: To run the Python file: Python file will run on Talon and show results just like it\u2019s locally: To see files on Talon in PyCharm click Remote Host on right side:","title":"PyCharm"},{"location":"pycharm/pycharm/#how-to-setup-pycharm-for-talon","text":"","title":"How to setup PyCharm for Talon"},{"location":"pycharm/pycharm/#apply-for-professional-account","text":"In order to register for a professional account use please go to: https://www.jetbrains.com/student/ Apply:","title":"Apply for professional account"},{"location":"pycharm/pycharm/#download-pycharm-professional","text":"Go to: https://www.jetbrains.com/pycharm/ Press Download: Choose Professional version [in this example I have use Linux. For Windows or Mac will be slightly different]:","title":"Download PyCharm Professional"},{"location":"pycharm/pycharm/#install-pycharm","text":"Follow normal installation instruction Fisrt time setup: Skip Remaining and Set Defaults Welcome to PyCharm Go to Settings Now click on Project Interpreter Click on right wheel Add\u2026 SSH Interpreter New server configuration: enter Talon Vis node address: vis-01.acs.unt.edu Asks for Connecting To Remote Host: Accept [It might ask for your password instead] It mgiht also ask you to enter password of your JetBrain account. You created this account earlier. [Make sure to enter password for that account] Default remote interpreter: Brows to add a new interpreter: Enter path of Talon Python interpreter: Set path interpreter: Path Interpreter should look like this now: Project Interpreter should look like this now: Need to wait for a while to finish:","title":"Install PyCharm:"},{"location":"pycharm/pycharm/#test-with-creating-a-new-project","text":"Create New Project Select Pure Python Choose preffered name of project. In this case is test Now we need to set interpreter: Click Project Interpreter > Existing Interpreter: In Remote project location make sure add the path on Talon where you want your project to be! Wait for some time: If no folder is created wiht project name in Talon, PyCharm will create it for you. Create Python file: Name your python test file: File uploaded to Talon when it\u2019s created: Write a print(\u201cHello world!\u201d) in python file. When save the file it will automatichally save it to Talon: To run the Python file: Python file will run on Talon and show results just like it\u2019s locally: To see files on Talon in PyCharm click Remote Host on right side:","title":"Test with creating a new project"},{"location":"python/code_samples/","text":"Python Useful Code Samples \u00b6 Here are a few Python code samples useful when you work on Talon HPC. All Python code is for Python 3! Reverse \u00b6 ''' reversing string with special case of slice step param ''' a = 'abcdefghijklmnopqrstuvwxyz' print ( a [:: - 1 ]) ''' iterating eover stirng reversly efficiantly ''' for char in reversed ( a ): print ( char ) print ([ 1 , 2 , 3 , 4 , 5 ][:: - 1 ]) List to String \u00b6 a = [ \"Python\" , \"is\" , \"awesome!\" ] print ( \" \" . join ( a )) >> Python is awesome ! Chained Functions \u00b6 def prod ( a , b ): return a * b def sum ( a , b ): return a + b a , b = 3 , 10 print (( prod if a < b else sum )( a , b )) >> 30 Checking Words are Anagrams \u00b6 # imports from collections import Counter str1 = \"cat\" str2 = \"tac\" Counter ( str1 ) == Counter ( str2 ) >> True Concatenate Lists \u00b6 a = [ 1 , 2 , 3 , 4 ] b = [ 5 , 6 , 7 ] c = [ * a , * b ] print ( c ) >> [ 1 , 2 , 3 , 4 , 5 , 6 , 7 ] Copy Lists \u00b6 from copy import deepcopy a = [[ 1 , 2 , 3 , 4 ], [ 1 , 2 , 54 , 76 , 87 ]] b = deepcopy ( a ) b [ 0 ][ 3 ] = 999 print ( a , b ) # good for nested listst >> [[ 1 , 2 , 3 , 4 ], [ 1 , 2 , 54 , 76 , 87 ]] [[ 1 , 2 , 3 , 999 ], [ 1 , 2 , 54 , 76 , 87 ]] Safe dictionary get \u00b6 a = { 1 : 'a' , 3 : 'v' , 99 : 'B' } print ( a . get ( 1 , 'None' )) print ( a . get ( 98 , 'None' )) >> a >> None Statement for-else \u00b6 a = [ 1 , 2 , 3 , 4 , 5 , 6 ] for i in a : if i == 0 : break else : print ( \"Did not reach loop!\" ) >> Did not reach loop ! Single-line for \u00b6 a = [ 1 , 2 , 3 , - 1 ] [ 0 if i < 0 else i for i in a ] >> [ 1 , 2 , 3 , 0 ] Merge dictionaries \u00b6 d1 = { 'a' : 1 } d2 = { 'b' : 34 } print ({ ** d1 , ** d2 }) >> { 'a' : 1 , 'b' : 34 } Sorting Dictionary \u00b6 mydict = { 'carl' : 40 , 'alan' : 2 , 'bob' : 1 , 'danny' : 0 } # How to sort a dict by value Python 3> sort = { key : value for key , value in sorted ( mydict . items (), key = lambda kv : ( kv [ 1 ], kv [ 0 ]))} print ( sort ) # How to sort a dict by key Python 3> sort = { key : mydict [ key ] for key in sorted ( mydict . keys ())} print ( sort ) >> { 'danny' : 0 , 'bob' : 1 , 'alan' : 2 , 'carl' : 40 } >> { 'alan' : 2 , 'bob' : 1 , 'carl' : 40 , 'danny' : 0 } \u00b6","title":"Code Samples"},{"location":"python/code_samples/#python-useful-code-samples","text":"Here are a few Python code samples useful when you work on Talon HPC. All Python code is for Python 3!","title":"Python Useful Code Samples"},{"location":"python/code_samples/#reverse","text":"''' reversing string with special case of slice step param ''' a = 'abcdefghijklmnopqrstuvwxyz' print ( a [:: - 1 ]) ''' iterating eover stirng reversly efficiantly ''' for char in reversed ( a ): print ( char ) print ([ 1 , 2 , 3 , 4 , 5 ][:: - 1 ])","title":"Reverse"},{"location":"python/code_samples/#list-to-string","text":"a = [ \"Python\" , \"is\" , \"awesome!\" ] print ( \" \" . join ( a )) >> Python is awesome !","title":"List to String"},{"location":"python/code_samples/#chained-functions","text":"def prod ( a , b ): return a * b def sum ( a , b ): return a + b a , b = 3 , 10 print (( prod if a < b else sum )( a , b )) >> 30","title":"Chained Functions"},{"location":"python/code_samples/#checking-words-are-anagrams","text":"# imports from collections import Counter str1 = \"cat\" str2 = \"tac\" Counter ( str1 ) == Counter ( str2 ) >> True","title":"Checking Words are Anagrams"},{"location":"python/code_samples/#concatenate-lists","text":"a = [ 1 , 2 , 3 , 4 ] b = [ 5 , 6 , 7 ] c = [ * a , * b ] print ( c ) >> [ 1 , 2 , 3 , 4 , 5 , 6 , 7 ]","title":"Concatenate Lists"},{"location":"python/code_samples/#copy-lists","text":"from copy import deepcopy a = [[ 1 , 2 , 3 , 4 ], [ 1 , 2 , 54 , 76 , 87 ]] b = deepcopy ( a ) b [ 0 ][ 3 ] = 999 print ( a , b ) # good for nested listst >> [[ 1 , 2 , 3 , 4 ], [ 1 , 2 , 54 , 76 , 87 ]] [[ 1 , 2 , 3 , 999 ], [ 1 , 2 , 54 , 76 , 87 ]]","title":"Copy Lists"},{"location":"python/code_samples/#safe-dictionary-get","text":"a = { 1 : 'a' , 3 : 'v' , 99 : 'B' } print ( a . get ( 1 , 'None' )) print ( a . get ( 98 , 'None' )) >> a >> None","title":"Safe dictionary get"},{"location":"python/code_samples/#statement-for-else","text":"a = [ 1 , 2 , 3 , 4 , 5 , 6 ] for i in a : if i == 0 : break else : print ( \"Did not reach loop!\" ) >> Did not reach loop !","title":"Statement for-else"},{"location":"python/code_samples/#single-line-for","text":"a = [ 1 , 2 , 3 , - 1 ] [ 0 if i < 0 else i for i in a ] >> [ 1 , 2 , 3 , 0 ]","title":"Single-line for"},{"location":"python/code_samples/#merge-dictionaries","text":"d1 = { 'a' : 1 } d2 = { 'b' : 34 } print ({ ** d1 , ** d2 }) >> { 'a' : 1 , 'b' : 34 }","title":"Merge dictionaries"},{"location":"python/code_samples/#sorting-dictionary","text":"mydict = { 'carl' : 40 , 'alan' : 2 , 'bob' : 1 , 'danny' : 0 } # How to sort a dict by value Python 3> sort = { key : value for key , value in sorted ( mydict . items (), key = lambda kv : ( kv [ 1 ], kv [ 0 ]))} print ( sort ) # How to sort a dict by key Python 3> sort = { key : mydict [ key ] for key in sorted ( mydict . keys ())} print ( sort ) >> { 'danny' : 0 , 'bob' : 1 , 'alan' : 2 , 'carl' : 40 } >> { 'alan' : 2 , 'bob' : 1 , 'carl' : 40 , 'danny' : 0 }","title":"Sorting Dictionary"},{"location":"python/code_samples/#_1","text":"","title":""},{"location":"python/jupyter_notebook/","text":"Jupyter Notebook on Talon HPC \u00b6 These are instructions on how to start a YOUR OWN Jupyter Notebook server on Talon and how to access it from your computer. First time ONLY \u00b6 Execute this step to setup password for your Jupyter Notebook. Login to a Visualization login nodes . Use ONE of the following: $ ssh YOUR_EUID@vis-01.acs.unt.edu $ ssh YOUR_EUID@vis-02.acs.unt.edu $ ssh YOUR_EUID@vis-03.acs.unt.edu Load the appropriate module: $ module load python/3.6.5 Type this in your terminal: $ jupyter notebook password Then enter a password. This will be the password necessary to access Jupyter Notebook. THIS IS NOT YOUR EUID PASSWORD. You will have to type twice, for: $ Enter password: And $ Verify password: After you entered the password twice you should see a message: [ NotebookPasswordApp ] Wrote hashed password to /home/YOUR_EUID/.jupyter/jupyter_notebook_config.json This creates a configuration file jupyter_notebook_config.json related to your password. Now you can run Jupyter Notebook on Talon in a few ways: Run on an interactive node \u00b6 Login to a Visualization login nodes . Use ONE of the following: $ ssh YOUR_EUID@vis-01.acs.unt.edu $ ssh YOUR_EUID@vis-02.acs.unt.edu $ ssh YOUR_EUID@vis-03.acs.unt.edu Load the appropriate module: $ module load python/3.6.5 Launch Jupyter Notebook: Enter the following command in your terminal: $ jupyter notebook This will spawn a Jupyter Notebook terminal window that looks like this: As long as you want to use Jupyter Notebook you need to keep this terminal open. If you close this terminal you will lose your Jupyter Notebook session and any unsaved information! Keep track of the port that the notebook is running on. in the window you see $ REFRESH ( 1 sec ) : http://localhost:8891/tree 8891 is the port number in this case. In your case it can be a different number! That will be YOUR_PORT_NUMBER Forward Jupyter Notebook to your local machine browser Now you need to open a new terminal window and use the following command: $ ssh -L YOUR_PORT_NUMBER:localhost:YOUR_PORT_NUMBER YOUR_EUDI@vis-01.acs.unt.edu In this example the port number is 8891, so I will use: ssh -L 8891:localhost:8891 EUID@vis-01.acs.unt.edu You will replace this number with the one generated in your Jupyter Notebook terminal window. This will forward the port of the Jupyter Notebook running on talon to your local machine. Double check\u2026 Now you should have 2 terminals running: One terminal with the Jupyter Notebook terminal window like in 1.3. Launch Jupyter Notebook Another terminal that you used to login with like in 1.4 Forward Jupyter Notebook to your local machine browser : $ ssh -L YOUR_PORT_NUMBER:localhost:YOUR_PORT_NUMBER YOUR_EUDI@vis-01.acs.unt.edu These 2 terminal are the ones keeping your Jupyter Notebook alive and running on your local machine! Access your Jupyter Notebook: Now to access the Jupyter Notebook, open any browser (Chrome,Mozilla etc) and type: http://localhost:YOUR_PORT_NUMBER In my case, my port number is 8891 so I will have to use: http://localhost:8891 You will see a webpage like this: This is where you will enter your password that you created when you used $ jupyter notebook password Now you are inside your Jupyer Notebook on your local machine browser that runs on Talon! Cool! This is how mine looks like after login: I only have bin folder in this image. Yours will be different, you will see all your directories from Talon. Run as a job file \u00b6 Login to Talon: $ ssh YOUR_EUID@talon3.hpc.unt.edu Create and configure your .job file: Now you will need to create a .job file just like a batch submisison job (Check this out if not familiar with job submissions on Talon) Your .job file should look like this: #!/bin/bash #SBATCH -p public #SBATCH --qos general #SBATCH -N 1 #SBATCH --ntasks-per-node=28 #SBATCH -C c6320 module load python/3.6.5 unset XDG_RUNTIME_DIR jupyter notebook --no-browser --ip = 0 .0.0.0 The first 6 lines are configurable to anything you want. See Talon 3 job submission for more details. You can add/remove resources or functionality to it just like with any job file. The part that is very important for Jupyter Notebook in the job file is: module load python/3.6.5 unset XDG_RUNTIME_DIR jupyter notebook --no-browser --ip=0.0.0.0 Once you have the job file ready, you can launch it just like any job file on talon $ sbatch YOUR_JOB_FILE.job This will launch your job supporting Jupyter Notebooks. We need to find the compute node where the notebooks is running. For this we will use the output file generated by the job file (For example: job_JOB_ID.out) should look like: [I 12:33:10.043 NotebookApp] Serving notebooks from local directory: /home/gm0234 [I 12:33:10.043 NotebookApp] The Jupyter Notebook is running at: [I 12:33:10.043 NotebookApp] http://(YOUR_NODE_ADDRESS or 127.0.0.1):YOUR_PORT_NUMBER/ [I 12:33:10.043 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation). As an example: [I 12:33:10.043 NotebookApp] Serving notebooks from local directory: /home/gm0234 [I 12:33:10.043 NotebookApp] The Jupyter Notebook is running at: [I 12:33:10.043 NotebookApp] http://(c32-9-29 or 127.0.0.1):8888/ [I 12:33:10.043 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation). The part we care about is: [I 12:33:10.043 NotebookApp] http://(c32-9-29 or 127.0.0.1):8888/ More exactly: (c32-9-29 or 127.0.0.1):8888/ From this we know the YOUR_NODE_ADDRESS : c32-9-29 And YOUR_PORT_NUMBER : 8888 Having these two (YOUR_NODE_ADDRESS and YOUR_PORT_NUMBER) we can open a new terminal and login to forward that address to your local machine: $ ssh -L YOUR_PORT_NUMBER:YOUR_NODE_ADDRESS:YOUR_PORT_NUMBER YOUR_EUID@vis-01.acs.unt.edu For this example: $ ssh -L 8888:c32-9-29:8888 YOUR_EUID@vis-01.acs.unt.edu Access your Jupyter Notebook: Now to access the Jupyter Notebook, open any browser (Chrome,Mozilla etc) and type: http://localhost:YOUR_PORT_NUMBER In my case, my port number is 8888 so I will have to use: http://localhost:8888 You will see a webpage like this: This is where you will enter your password that you created when you used $ jupyter notebook password Now you are inside your Jupyer Notebook on your local machine browser that runs on Talon! Cool! This is how mine looks like after login: I only have bin folder in this image. Yours will be different, you will see all your directories from Talon. Notebooks on GPUs (use GPUs on your notebook) \u00b6 Follow same steps as Run as a job file but use a different .job file: #!/bin/bash #SBATCH -J jupyter #SBATCH -o job_%j.out #SBATCH -p gpu #SBATCH --qos general #SBATCH -N 1 #SBATCH --gres=gpu:4 #SBATCH --mail-type=begin #SBATCH --mail-user=username@my.unt.edu module load pytorch/1.0.1 unset XDG_RUNTIME_DIR jupyter notebook --no-browser --ip = 0 .0.0.0 The first 6 lines are configurable to anything you want. See Talon 3 job submission for more details. You can add/remove resources or functionality to it just like with any job file.","title":"Jupyter Notebook on Talon HPC"},{"location":"python/jupyter_notebook/#jupyter-notebook-on-talon-hpc","text":"These are instructions on how to start a YOUR OWN Jupyter Notebook server on Talon and how to access it from your computer.","title":"Jupyter Notebook on Talon HPC"},{"location":"python/jupyter_notebook/#first-time-only","text":"Execute this step to setup password for your Jupyter Notebook. Login to a Visualization login nodes . Use ONE of the following: $ ssh YOUR_EUID@vis-01.acs.unt.edu $ ssh YOUR_EUID@vis-02.acs.unt.edu $ ssh YOUR_EUID@vis-03.acs.unt.edu Load the appropriate module: $ module load python/3.6.5 Type this in your terminal: $ jupyter notebook password Then enter a password. This will be the password necessary to access Jupyter Notebook. THIS IS NOT YOUR EUID PASSWORD. You will have to type twice, for: $ Enter password: And $ Verify password: After you entered the password twice you should see a message: [ NotebookPasswordApp ] Wrote hashed password to /home/YOUR_EUID/.jupyter/jupyter_notebook_config.json This creates a configuration file jupyter_notebook_config.json related to your password. Now you can run Jupyter Notebook on Talon in a few ways:","title":"First time ONLY"},{"location":"python/jupyter_notebook/#run-on-an-interactive-node","text":"Login to a Visualization login nodes . Use ONE of the following: $ ssh YOUR_EUID@vis-01.acs.unt.edu $ ssh YOUR_EUID@vis-02.acs.unt.edu $ ssh YOUR_EUID@vis-03.acs.unt.edu Load the appropriate module: $ module load python/3.6.5 Launch Jupyter Notebook: Enter the following command in your terminal: $ jupyter notebook This will spawn a Jupyter Notebook terminal window that looks like this: As long as you want to use Jupyter Notebook you need to keep this terminal open. If you close this terminal you will lose your Jupyter Notebook session and any unsaved information! Keep track of the port that the notebook is running on. in the window you see $ REFRESH ( 1 sec ) : http://localhost:8891/tree 8891 is the port number in this case. In your case it can be a different number! That will be YOUR_PORT_NUMBER Forward Jupyter Notebook to your local machine browser Now you need to open a new terminal window and use the following command: $ ssh -L YOUR_PORT_NUMBER:localhost:YOUR_PORT_NUMBER YOUR_EUDI@vis-01.acs.unt.edu In this example the port number is 8891, so I will use: ssh -L 8891:localhost:8891 EUID@vis-01.acs.unt.edu You will replace this number with the one generated in your Jupyter Notebook terminal window. This will forward the port of the Jupyter Notebook running on talon to your local machine. Double check\u2026 Now you should have 2 terminals running: One terminal with the Jupyter Notebook terminal window like in 1.3. Launch Jupyter Notebook Another terminal that you used to login with like in 1.4 Forward Jupyter Notebook to your local machine browser : $ ssh -L YOUR_PORT_NUMBER:localhost:YOUR_PORT_NUMBER YOUR_EUDI@vis-01.acs.unt.edu These 2 terminal are the ones keeping your Jupyter Notebook alive and running on your local machine! Access your Jupyter Notebook: Now to access the Jupyter Notebook, open any browser (Chrome,Mozilla etc) and type: http://localhost:YOUR_PORT_NUMBER In my case, my port number is 8891 so I will have to use: http://localhost:8891 You will see a webpage like this: This is where you will enter your password that you created when you used $ jupyter notebook password Now you are inside your Jupyer Notebook on your local machine browser that runs on Talon! Cool! This is how mine looks like after login: I only have bin folder in this image. Yours will be different, you will see all your directories from Talon.","title":"Run on an interactive node"},{"location":"python/jupyter_notebook/#run-as-a-job-file","text":"Login to Talon: $ ssh YOUR_EUID@talon3.hpc.unt.edu Create and configure your .job file: Now you will need to create a .job file just like a batch submisison job (Check this out if not familiar with job submissions on Talon) Your .job file should look like this: #!/bin/bash #SBATCH -p public #SBATCH --qos general #SBATCH -N 1 #SBATCH --ntasks-per-node=28 #SBATCH -C c6320 module load python/3.6.5 unset XDG_RUNTIME_DIR jupyter notebook --no-browser --ip = 0 .0.0.0 The first 6 lines are configurable to anything you want. See Talon 3 job submission for more details. You can add/remove resources or functionality to it just like with any job file. The part that is very important for Jupyter Notebook in the job file is: module load python/3.6.5 unset XDG_RUNTIME_DIR jupyter notebook --no-browser --ip=0.0.0.0 Once you have the job file ready, you can launch it just like any job file on talon $ sbatch YOUR_JOB_FILE.job This will launch your job supporting Jupyter Notebooks. We need to find the compute node where the notebooks is running. For this we will use the output file generated by the job file (For example: job_JOB_ID.out) should look like: [I 12:33:10.043 NotebookApp] Serving notebooks from local directory: /home/gm0234 [I 12:33:10.043 NotebookApp] The Jupyter Notebook is running at: [I 12:33:10.043 NotebookApp] http://(YOUR_NODE_ADDRESS or 127.0.0.1):YOUR_PORT_NUMBER/ [I 12:33:10.043 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation). As an example: [I 12:33:10.043 NotebookApp] Serving notebooks from local directory: /home/gm0234 [I 12:33:10.043 NotebookApp] The Jupyter Notebook is running at: [I 12:33:10.043 NotebookApp] http://(c32-9-29 or 127.0.0.1):8888/ [I 12:33:10.043 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation). The part we care about is: [I 12:33:10.043 NotebookApp] http://(c32-9-29 or 127.0.0.1):8888/ More exactly: (c32-9-29 or 127.0.0.1):8888/ From this we know the YOUR_NODE_ADDRESS : c32-9-29 And YOUR_PORT_NUMBER : 8888 Having these two (YOUR_NODE_ADDRESS and YOUR_PORT_NUMBER) we can open a new terminal and login to forward that address to your local machine: $ ssh -L YOUR_PORT_NUMBER:YOUR_NODE_ADDRESS:YOUR_PORT_NUMBER YOUR_EUID@vis-01.acs.unt.edu For this example: $ ssh -L 8888:c32-9-29:8888 YOUR_EUID@vis-01.acs.unt.edu Access your Jupyter Notebook: Now to access the Jupyter Notebook, open any browser (Chrome,Mozilla etc) and type: http://localhost:YOUR_PORT_NUMBER In my case, my port number is 8888 so I will have to use: http://localhost:8888 You will see a webpage like this: This is where you will enter your password that you created when you used $ jupyter notebook password Now you are inside your Jupyer Notebook on your local machine browser that runs on Talon! Cool! This is how mine looks like after login: I only have bin folder in this image. Yours will be different, you will see all your directories from Talon.","title":"Run as a job file"},{"location":"python/jupyter_notebook/#notebooks-on-gpus-use-gpus-on-your-notebook","text":"Follow same steps as Run as a job file but use a different .job file: #!/bin/bash #SBATCH -J jupyter #SBATCH -o job_%j.out #SBATCH -p gpu #SBATCH --qos general #SBATCH -N 1 #SBATCH --gres=gpu:4 #SBATCH --mail-type=begin #SBATCH --mail-user=username@my.unt.edu module load pytorch/1.0.1 unset XDG_RUNTIME_DIR jupyter notebook --no-browser --ip = 0 .0.0.0 The first 6 lines are configurable to anything you want. See Talon 3 job submission for more details. You can add/remove resources or functionality to it just like with any job file.","title":"Notebooks on GPUs (use GPUs on your notebook)"},{"location":"python/python_info/","text":"How to start with Python on Talon HPC \u00b6 In order to use python you need to load it using Environment modules . Here are modules available for python. Use module load followed by the version of python you want to use (i.e. to use python 3.6.5 module load python/3.6.5 ): Python Modules pyMPI/2.4 python/2.7.13 python/2.7.14-pyCUDA python/3.5.7 python/3.6.0 python/3.6.0-2 python/3.6.5 python/3.7.0 python/3.7.4 jupyter/4.4.0 tensorflow/1.2.0 tensorflow/1.10.1-gpu tensorflow/1.10.1 tensorflow/1.12.3-gpu tensorflow/2.0 tensorflow/2.1.0-gpu keras/2.1.6 keras/2.2.0 h2o4pu/0.3.2 jupyter/jupyter/4.4.0","title":"Info"},{"location":"python/python_info/#how-to-start-with-python-on-talon-hpc","text":"In order to use python you need to load it using Environment modules . Here are modules available for python. Use module load followed by the version of python you want to use (i.e. to use python 3.6.5 module load python/3.6.5 ): Python Modules pyMPI/2.4 python/2.7.13 python/2.7.14-pyCUDA python/3.5.7 python/3.6.0 python/3.6.0-2 python/3.6.5 python/3.7.0 python/3.7.4 jupyter/4.4.0 tensorflow/1.2.0 tensorflow/1.10.1-gpu tensorflow/1.10.1 tensorflow/1.12.3-gpu tensorflow/2.0 tensorflow/2.1.0-gpu keras/2.1.6 keras/2.2.0 h2o4pu/0.3.2 jupyter/jupyter/4.4.0","title":"How to start with Python on Talon HPC"},{"location":"python/pytorch/","text":"We\u2019re working hard here! \u00b6","title":"PyTorch"},{"location":"python/pytorch/#were-working-hard-here","text":"","title":"We're working hard here!"},{"location":"python/tensorflow/","text":"We\u2019re working hard here! \u00b6","title":"Tensorflow"},{"location":"python/tensorflow/#were-working-hard-here","text":"","title":"We're working hard here!"},{"location":"r/r_info/","text":"How to start with R on Talon HPC \u00b6","title":"How to start with `R` on Talon HPC"},{"location":"r/r_info/#how-to-start-with-r-on-talon-hpc","text":"","title":"How to start with R on Talon HPC"},{"location":"r/rstudio/","text":"Using Rstudio on Talon3 \u00b6 Intro \u00b6 RStudio is a development IDE for R. This is a great resource for statistical computing and visulization. Talon3 supports various RStudio servers and is available on the Visualization nodes of Talon3. This means that you can use the RStudio interface to use Talon3!! Logging into RStudio \u00b6 You can access an RStudio interface via a web browser (Chrome, Firefox, Safari) by going to one of Talon3\u2019s Visualization nodes http://vis.acs.unt.edu:8787 htpp://vis-06.acs.unt.edu:8787 http://vis-07.acs.unt.edu:8787 The login credentials are the same as your Talon3 login/password Once you login, you have access to RStudio on the Talon3 network. You can execute simple R scripts, transer files, and visualizae data on Talon3. Since RStudio is running on the Visualization login nodes, any computationally intensive processes are prohibited and can result in expulsion from using UNT HPC resouces. Any large R tasks and commands MUST be submitted as a SLURM batch job. Tip Remember: Rsudio access, like SSH access, MUST be though the UNT Network. This means that if you are trying to access Rsudio outside of UNT, you MUST be connected to the UNT VPN network Using Rsudio on Talon3 \u00b6 Warning We are still working on updating our docs! Check back for more information! Please email SciComp-Support@unt.edu for your questions!","title":"Rstudio"},{"location":"r/rstudio/#using-rstudio-on-talon3","text":"","title":"Using Rstudio on Talon3"},{"location":"r/rstudio/#intro","text":"RStudio is a development IDE for R. This is a great resource for statistical computing and visulization. Talon3 supports various RStudio servers and is available on the Visualization nodes of Talon3. This means that you can use the RStudio interface to use Talon3!!","title":"Intro"},{"location":"r/rstudio/#logging-into-rstudio","text":"You can access an RStudio interface via a web browser (Chrome, Firefox, Safari) by going to one of Talon3\u2019s Visualization nodes http://vis.acs.unt.edu:8787 htpp://vis-06.acs.unt.edu:8787 http://vis-07.acs.unt.edu:8787 The login credentials are the same as your Talon3 login/password Once you login, you have access to RStudio on the Talon3 network. You can execute simple R scripts, transer files, and visualizae data on Talon3. Since RStudio is running on the Visualization login nodes, any computationally intensive processes are prohibited and can result in expulsion from using UNT HPC resouces. Any large R tasks and commands MUST be submitted as a SLURM batch job. Tip Remember: Rsudio access, like SSH access, MUST be though the UNT Network. This means that if you are trying to access Rsudio outside of UNT, you MUST be connected to the UNT VPN network","title":"Logging into RStudio"},{"location":"r/rstudio/#using-rsudio-on-talon3","text":"Warning We are still working on updating our docs! Check back for more information! Please email SciComp-Support@unt.edu for your questions!","title":"Using Rsudio on Talon3"},{"location":"r/under_construction/","text":"We\u2019re working hard here! \u00b6","title":"We're working hard here!"},{"location":"r/under_construction/#were-working-hard-here","text":"","title":"We're working hard here!"},{"location":"sci_soft/containers/","text":"Talon3 supports OS level virtualization (containers). Currently, Talon3 supports Singularity to run containers. Using Singularity \u00b6 Talon3 has Singularity version 3.6 installed. To use singulartiy on Talon, first you MUST load the module module load singularity/3.6 Building a container \u00b6 Users can set up Singularity on thier own workstation and create an container as a SIF image. The container can be used to run as a job on Talon. Talon3 supported containers \u00b6 There are Talon3 supported containers for users located at /storage/scratch2/share/containers Talon3 supported containers: biogrinder Example container job \u00b6 module load singularity/3.6 Pulling ubuntu image from dockerhub singularity pull docker://ubuntu:20.04 Running container singularity exec --bind /storage/scratch2/$USER:/storage/scratch2/$USER,/home/$USER/:/home/$USER/ -e ubuntu_20.04.sif python test.py","title":"Containers"},{"location":"sci_soft/containers/#using-singularity","text":"Talon3 has Singularity version 3.6 installed. To use singulartiy on Talon, first you MUST load the module module load singularity/3.6","title":"Using Singularity"},{"location":"sci_soft/containers/#building-a-container","text":"Users can set up Singularity on thier own workstation and create an container as a SIF image. The container can be used to run as a job on Talon.","title":"Building a container"},{"location":"sci_soft/containers/#talon3-supported-containers","text":"There are Talon3 supported containers for users located at /storage/scratch2/share/containers Talon3 supported containers: biogrinder","title":"Talon3 supported containers"},{"location":"sci_soft/containers/#example-container-job","text":"module load singularity/3.6 Pulling ubuntu image from dockerhub singularity pull docker://ubuntu:20.04 Running container singularity exec --bind /storage/scratch2/$USER:/storage/scratch2/$USER,/home/$USER/:/home/$USER/ -e ubuntu_20.04.sif python test.py","title":"Example container job"},{"location":"sci_soft/sci_soft/","text":"Talon3 supports a vast array of scientific applications. This is a multi-interdisciplinary list of state-of-the-art software that is optimized by HPC staff to best take advantage of Talon3 resources. The HPC staff has build all applications to give high throughput. Biology, Chemistry, Material Science, Physics \u00b6 Name AMBER Armadillo BamTools Beast BLAST+ Bowtie2 BWA Cairo CD-Hit dDocent DL_PLOY Ea-Utils Eigen Espresso FastxToolkit GAMESS Gatk Gaussian Gromacs Hisat HTSlib ICU Kallisto Kraken LAMMPS Minimcas2 Mpiblast Namd NWChem Oases Openfoam Ovito Picard Plink PSI4 RAxML SAMTOOLS seqprep SOAPdenovo Stringtie TopHat Trinity VASP Velvet VMD WARP Programming Laguages \u00b6 Name Intel GCC Python R Perl Java Juila Go Mathematica MATLAB Programming Tools \u00b6 Name Anaconda Bazel Boost CMake FFTW glibc GNU Plot GSL hadoop HDF5 Lapack MKL OPENMPI PSSH pyMPI Readline SageMath Star Szip Visit Zlib","title":"Scientific Software"},{"location":"sci_soft/sci_soft/#biology-chemistry-material-science-physics","text":"Name AMBER Armadillo BamTools Beast BLAST+ Bowtie2 BWA Cairo CD-Hit dDocent DL_PLOY Ea-Utils Eigen Espresso FastxToolkit GAMESS Gatk Gaussian Gromacs Hisat HTSlib ICU Kallisto Kraken LAMMPS Minimcas2 Mpiblast Namd NWChem Oases Openfoam Ovito Picard Plink PSI4 RAxML SAMTOOLS seqprep SOAPdenovo Stringtie TopHat Trinity VASP Velvet VMD WARP","title":"Biology, Chemistry, Material Science, Physics"},{"location":"sci_soft/sci_soft/#programming-laguages","text":"Name Intel GCC Python R Perl Java Juila Go Mathematica MATLAB","title":"Programming Laguages"},{"location":"sci_soft/sci_soft/#programming-tools","text":"Name Anaconda Bazel Boost CMake FFTW glibc GNU Plot GSL hadoop HDF5 Lapack MKL OPENMPI PSSH pyMPI Readline SageMath Star Szip Visit Zlib","title":"Programming Tools"},{"location":"terminal/commands/","text":"Commands \u00b6 Here are a few commands which are useful when using Talon3 HPC: Modules: \u00b6 See available modules: $ module avail Load modules: $ module load name_of_module Unload module: $ module unload unload name_of_module Pre-Load modules when login to Talon \u00b6 If you have modules you\u2019re using everytime you can pre-load them everytime you login to Talon. This way you avaoid having to do module load everytime. Terminal commands: $ cd $ vim .bashrc Now you opened the bash file and you should see: module load gcc slurm On the same line add any module you want loaded when ever you login to Talon. For example, to load python 3.6.5 anytime we login: module load gcc slurm python/3.6.5 Job details: \u00b6 Find out your job id: $ squeue -u euid123 $JOB_ID can be seen under JOBID column: $ scontrol show job $JOB_ID Kill a job. Users can kill their own jobs, root can kill any job: $ scancel $JOB_ID Hold a job: $ scontrol hold $JOB_ID Release a job: $ scontrol release $JOB_ID See Resources: \u00b6 Login to your node: \u00b6 Find out your node name: $ squeue -u euid123 $NODE_NAME can be seen under NODELIST column To login your node: $ slogin $NODE_NAME Once logged in the node, use following commands: \u00b6 See load on CPUs: $ mpstat -P ALL See process running on CPUs: $ pidstat See GPU resources [IF IS A GPU NODE]: $ nvidia-smi Show breakdown of jobs by user \u00b6 $ squeue -h | awk '{print $4}' | sort | uniq -c | sort -rn Show queue statue by a particular user \u00b6 $ squeue -u euid123 Code on your computer and run it straight to an alocated node on Talon: \u00b6 This can be very usefull when you have code on your personal computer that you want to run on Talon. You will need your $NODE_NAME for this. You run this command from your personal computer\u2019s terminal! $ ssh euid123@talon3.hpc.unt.edu slogin $NODE_NAME python < your_local_file.py your_local_file.py is a file located on your personal computer. If you require certain modules to be loaded, pre-load them in your .bashrc file - See previous. If you setup your ssh shortcut it will be as easy as: $ ssh t3 slogin $NODE_NAME python < your_local_file.py If you need to use a dataset, you will need to change directories when ssh. To do this you have to cd first in the desired directory and then run the file: $ ssh t3 slogin $NODE_NAME \"cd /home/euid/your_path ; python\" < your_local_file.py NOTE: Remember to add any modules you want pre-loaded if you use this command! \u00b6","title":"Commands"},{"location":"terminal/commands/#commands","text":"Here are a few commands which are useful when using Talon3 HPC:","title":"Commands"},{"location":"terminal/commands/#modules","text":"See available modules: $ module avail Load modules: $ module load name_of_module Unload module: $ module unload unload name_of_module","title":"Modules:"},{"location":"terminal/commands/#pre-load-modules-when-login-to-talon","text":"If you have modules you\u2019re using everytime you can pre-load them everytime you login to Talon. This way you avaoid having to do module load everytime. Terminal commands: $ cd $ vim .bashrc Now you opened the bash file and you should see: module load gcc slurm On the same line add any module you want loaded when ever you login to Talon. For example, to load python 3.6.5 anytime we login: module load gcc slurm python/3.6.5","title":"Pre-Load modules when login to Talon"},{"location":"terminal/commands/#job-details","text":"Find out your job id: $ squeue -u euid123 $JOB_ID can be seen under JOBID column: $ scontrol show job $JOB_ID Kill a job. Users can kill their own jobs, root can kill any job: $ scancel $JOB_ID Hold a job: $ scontrol hold $JOB_ID Release a job: $ scontrol release $JOB_ID","title":"Job details:"},{"location":"terminal/commands/#see-resources","text":"","title":"See Resources:"},{"location":"terminal/commands/#login-to-your-node","text":"Find out your node name: $ squeue -u euid123 $NODE_NAME can be seen under NODELIST column To login your node: $ slogin $NODE_NAME","title":"Login to your node:"},{"location":"terminal/commands/#once-logged-in-the-node-use-following-commands","text":"See load on CPUs: $ mpstat -P ALL See process running on CPUs: $ pidstat See GPU resources [IF IS A GPU NODE]: $ nvidia-smi","title":"Once logged in the node, use following commands:"},{"location":"terminal/commands/#show-breakdown-of-jobs-by-user","text":"$ squeue -h | awk '{print $4}' | sort | uniq -c | sort -rn","title":"Show breakdown of jobs by user"},{"location":"terminal/commands/#show-queue-statue-by-a-particular-user","text":"$ squeue -u euid123","title":"Show queue statue by a particular user"},{"location":"terminal/commands/#code-on-your-computer-and-run-it-straight-to-an-alocated-node-on-talon","text":"This can be very usefull when you have code on your personal computer that you want to run on Talon. You will need your $NODE_NAME for this. You run this command from your personal computer\u2019s terminal! $ ssh euid123@talon3.hpc.unt.edu slogin $NODE_NAME python < your_local_file.py your_local_file.py is a file located on your personal computer. If you require certain modules to be loaded, pre-load them in your .bashrc file - See previous. If you setup your ssh shortcut it will be as easy as: $ ssh t3 slogin $NODE_NAME python < your_local_file.py If you need to use a dataset, you will need to change directories when ssh. To do this you have to cd first in the desired directory and then run the file: $ ssh t3 slogin $NODE_NAME \"cd /home/euid/your_path ; python\" < your_local_file.py","title":"Code on your computer and run it straight to an alocated node on Talon:"},{"location":"terminal/commands/#note-remember-to-add-any-modules-you-want-pre-loaded-if-you-use-this-command","text":"","title":"NOTE: Remember to add any modules you want pre-loaded if you use this command!"},{"location":"terminal/screen/","text":"How to use screen \u00b6 Find more info here . Talon 3 comes with screen installed. It is very useful to use when you want to close a terminal session without interrupting (especially when you use Job Interactive). After you login to Talon \u00b6 $ ssh euid123@talon3.hpc.unt.edu You can type: $ screen --help To find out some commands. To create a new screen: \u00b6 $ screen -S screen_name It will open up a new terminal window in the same current window with the screen_name ( it can be any name you want ) you specified. Make sure to run: $ source ~/.bashrc To have all the environment variables loaded! Here you can request a node, or just run any code. Once you want to log-out/ detach , but keep the terminal running just type: CTRL + a + d \u00b6 It will detach from the terminal and come back to your previous terminal. You can have as many screens setup as you want. To see all your screens type: \u00b6 $ screen -ls To login back in one of the screens (attach): \u00b6 $ screen -r screen_name Create a new screen and detach: \u00b6 $ screen -dmS screen name This is very useful when you are doing an interactive job. srun + screen + detach \u00b6 CPU Screen \u00b6 $ screen -dmS screen_name srun -p public --qos general --mail-user = user@unt.edu --mail-type = ALL -N 1 --pty bash Opens up a screen, runs the srun command with email notification and then it detaches. Now you just need to wait for email notification when node was allocated so you can screen back in using the screen_name you provided. This way you can shutdown your computer, or move to a different computer. It is also very useful when you have a bad internet connection, you can lose your place in the queue if terminal session ends! GPU Screen \u00b6 $ screen -dmS gpu_screen srun -p gpu --mail-user = user@unt.edu --mail-type = ALL --gres = gpu:4 -N 1 --pty bash","title":"How to use screen"},{"location":"terminal/screen/#how-to-use-screen","text":"Find more info here . Talon 3 comes with screen installed. It is very useful to use when you want to close a terminal session without interrupting (especially when you use Job Interactive).","title":"How to use screen"},{"location":"terminal/screen/#after-you-login-to-talon","text":"$ ssh euid123@talon3.hpc.unt.edu You can type: $ screen --help To find out some commands.","title":"After you login to Talon"},{"location":"terminal/screen/#to-create-a-new-screen","text":"$ screen -S screen_name It will open up a new terminal window in the same current window with the screen_name ( it can be any name you want ) you specified. Make sure to run: $ source ~/.bashrc To have all the environment variables loaded! Here you can request a node, or just run any code. Once you want to log-out/ detach , but keep the terminal running just type:","title":"To create a new screen:"},{"location":"terminal/screen/#ctrl-a-d","text":"It will detach from the terminal and come back to your previous terminal. You can have as many screens setup as you want.","title":"CTRL + a + d"},{"location":"terminal/screen/#to-see-all-your-screens-type","text":"$ screen -ls","title":"To see all your screens type:"},{"location":"terminal/screen/#to-login-back-in-one-of-the-screens-attach","text":"$ screen -r screen_name","title":"To login back in one of the screens (attach):"},{"location":"terminal/screen/#create-a-new-screen-and-detach","text":"$ screen -dmS screen name This is very useful when you are doing an interactive job.","title":"Create a new screen and detach:"},{"location":"terminal/screen/#srun-screen-detach","text":"","title":"srun + screen + detach"},{"location":"terminal/screen/#cpu-screen","text":"$ screen -dmS screen_name srun -p public --qos general --mail-user = user@unt.edu --mail-type = ALL -N 1 --pty bash Opens up a screen, runs the srun command with email notification and then it detaches. Now you just need to wait for email notification when node was allocated so you can screen back in using the screen_name you provided. This way you can shutdown your computer, or move to a different computer. It is also very useful when you have a bad internet connection, you can lose your place in the queue if terminal session ends!","title":"CPU Screen"},{"location":"terminal/screen/#gpu-screen","text":"$ screen -dmS gpu_screen srun -p gpu --mail-user = user@unt.edu --mail-type = ALL --gres = gpu:4 -N 1 --pty bash","title":"GPU Screen"},{"location":"terminal/ssh/","text":"SSH Terminal Easy Configuration \u00b6 Connecting from a Windows Computer \u00b6 If you are using a Windows OS personal computer, you will need to download a SSH client application that supports SSH-2 protocol. Examples of these SSH client programs you can freely download are PuTTY , OpenSSH , Cygwin/X , and SecureCRT You can also use other terminal approaches like using Windows Subsytem for Linux . This will allow you to run a Linux environment from inside your Windows OS. You can then, for example, the Ubuntu app from the Microsoft Store. Once you have installed a SSH client app, you can connect to Talon3 by entering in the Hostname of the login Talon3 nodes : talon3.hpc.unt.edu Connecting from a Mac or Linux Computer \u00b6 If you are logging from a Linux or Mac personal computer, make sure it has an SSH client (ssh, openssh, etc.) installed. Then access Talon3 by opening a Terminal application. If you never used SSH before this is a helpful example. Or if you don\u2019t know if you have SSH installed on your system, click here for Mac and here for Linux Examples of using termainal apps \u00b6 Warning We are still working on updating our docs! Check back for more information! Please email SciComp-Support@unt.edu for your questions!","title":"SSH Terminal"},{"location":"terminal/ssh/#ssh-terminal-easy-configuration","text":"","title":"SSH Terminal Easy Configuration"},{"location":"terminal/ssh/#connecting-from-a-windows-computer","text":"If you are using a Windows OS personal computer, you will need to download a SSH client application that supports SSH-2 protocol. Examples of these SSH client programs you can freely download are PuTTY , OpenSSH , Cygwin/X , and SecureCRT You can also use other terminal approaches like using Windows Subsytem for Linux . This will allow you to run a Linux environment from inside your Windows OS. You can then, for example, the Ubuntu app from the Microsoft Store. Once you have installed a SSH client app, you can connect to Talon3 by entering in the Hostname of the login Talon3 nodes : talon3.hpc.unt.edu","title":"Connecting from a Windows Computer"},{"location":"terminal/ssh/#connecting-from-a-mac-or-linux-computer","text":"If you are logging from a Linux or Mac personal computer, make sure it has an SSH client (ssh, openssh, etc.) installed. Then access Talon3 by opening a Terminal application. If you never used SSH before this is a helpful example. Or if you don\u2019t know if you have SSH installed on your system, click here for Mac and here for Linux","title":"Connecting from a Mac or Linux Computer"},{"location":"terminal/ssh/#examples-of-using-termainal-apps","text":"Warning We are still working on updating our docs! Check back for more information! Please email SciComp-Support@unt.edu for your questions!","title":"Examples of using termainal apps"},{"location":"tutorials/getting_started/","text":"Get started on Talon \u00b6 How to run a short .py code on HPC (Talon 3) \u00b6 For Mac OS/Linux Ubuntu \u00b6 1. Request an HPC account: \u00b6 If you already have one, you are good to go; If you don\u2019t have one, please visit https://hpc.unt.edu/account-request ; Read carefully and make sure you have all information requested: When you are ready, go to https://hpc.unt.edu/user ; Login in with your EUID (6 characters: initials+4 digits); Fill in the form and wait for our email. Make sure you are connected to the UNT VPN: Instructions can be found here https://itservices.cas.unt.edu/services/accounts-servers/articles/cisco-anyconnect-mobility-client-vpn 2. Open terminal/command prompt \u00b6 3. Login to your HPC account by typing: ssh youraccount@talon3.hpc.unt.edu , press Enter and you\u2019ll be asked to insert password as bellow: \u00b6 4. Fill in with your password, hit Enter and you are in! \u00b6 5. Environment setup \u00b6 Before running any kind of code, you need to make sure you have the right environment set up. For this, type \u201cmodule avail\u201d and press Enter. This will give you something like in the following picture where you can check all modules that are available on HPC. 6. Lookup module \u00b6 Look for the module you need. Let\u2019s say you want to run your code using Python 3.6.5. I will type: module load python/3.6.5 because that\u2019s the name in the list of modules available (see the bottom of the picture above) 7. Upload file to Talon \u00b6 Now that it\u2019s all set up you only need to upload your file containing the python code 8. Access user folder \u00b6 You need to access your user folder from HPC. To do that type in: cd /storage/scratch2/youraccount press Enter Hint: if you want to check what subfolders or files you have here you can type ll (small L\u2019s) 9. Create project folder \u00b6 I will create a folder for this project I want to run just to be organized mkdir functions you can replace word functions with the desired name for your folder Hint: avoid spaces, don\u2019t give names like \u201cProject 1\u201d, instead you can say \u201cProject_1\u201d after creating the folder I used the ll or ls command to check my files and folders from where I am at the moment 10. Access your new folder \u00b6 Access your new folder where the magic will happen: cd functions/ 11. Copy local to HPC \u00b6 Now, you want to copy your .py file from local computer on talon in the new folder created. For this, open a new terminal window without closing this one. Use the command cd to access the path of the file you want to transfer. For example: cd Downloads/ But, if you are not sure where you are at you can type \u201cls\u201d first: I know my file is in Downloads folder so I\u2019ll go there by typing \u201ccd Downloads/\u201d I used again the \u201cls\u201d command to see all the files in the folder, but what I am interested in is functions_comparison.py Now, to transfer this file I\u2019ll do the following: scp functions_comparison.py lp0348@talon3.hpc.unt.edu :/storage/scratch2/lp0348/functions Notice the command is: scp filename youraccount@talon3.hpc.unt.edu :/the/folder/path/on/talon After introducing your HPC account password you should see something like: Notice there\u2019s a percentage indicating the transfer progress Go back to the previous terminal window you were working on. You can check if the file was transferred: Here it is! Step 12. Time to create a batch file What is a batch file? \u2013 It is a script containing code running details related to resources you want to allocate from HPC. Detailed specifications on each line you can find here: https://github.com/gmihaila/unt_hpc/tree/master/job_batch For our example we will use a basic configuration. #!/bin/bash #SBATCH -J job_name #SBATCH -o output_job.o%j ## <- can be left like that (creates a file with the output) #SBATCH -e error_job.e%j. ## <- can be left like that (creates a file with the errors occurred) #SBATCH -p public #SBATCH --qos general #SBATCH -N 1 #SBATCH -n 1 #SBATCH -C c6320 module load python python test.py ## <- name of the .py code file We need to create a file .job where you put the above information. In this tutorial we will use VIM: The command: vim name_of_your_job.job After running this line you should see: This automatically created the batch file .job and opened it. In order to insert the content we discussed above, first hit \u201ci\u201d button from the keyboard. Notice the bottom changed to \u201c\u2014INSERT\u2014\u201d Now, you can type each line from scratch or you can paste it directly. But, if you are using Linux Ctrl+Shift+V would be the command to paste some content; if you are using Mac Command+V as usual will do the work. It might be easier for the beginning to write the batch content elsewhere and then just copy-paste the final commands. To save and quit the batch file: press \u201cEsc\u201d then type \u201c:wq\u201d Now \u201cEnter\u201d and will bring you back to the folder. 13. Run the job! \u00b6 The command line is: sbatch job_name.job You\u2019d probably get the message: It\u2019s done! You submitted a job that runs the py code on HPC. If you want to check your jobs you can use the command: squeue -u your_account If all your jobs are done already then squeue -u your_account command will give you the above output. From Windows \u00b6","title":"Get started on Talon"},{"location":"tutorials/getting_started/#get-started-on-talon","text":"","title":"Get started on Talon"},{"location":"tutorials/getting_started/#how-to-run-a-short-py-code-on-hpc-talon-3","text":"","title":"How to run a short .py code on HPC (Talon 3)"},{"location":"tutorials/getting_started/#for-mac-oslinux-ubuntu","text":"","title":"For  Mac OS/Linux Ubuntu"},{"location":"tutorials/getting_started/#1-request-an-hpc-account","text":"If you already have one, you are good to go; If you don\u2019t have one, please visit https://hpc.unt.edu/account-request ; Read carefully and make sure you have all information requested: When you are ready, go to https://hpc.unt.edu/user ; Login in with your EUID (6 characters: initials+4 digits); Fill in the form and wait for our email. Make sure you are connected to the UNT VPN: Instructions can be found here https://itservices.cas.unt.edu/services/accounts-servers/articles/cisco-anyconnect-mobility-client-vpn","title":"1. Request an HPC account:"},{"location":"tutorials/getting_started/#2-open-terminalcommand-prompt","text":"","title":"2. Open terminal/command prompt"},{"location":"tutorials/getting_started/#3-login-to-your-hpc-account-by-typing-ssh-youraccounttalon3hpcuntedu-press-enter-and-youll-be-asked-to-insert-password-as-bellow","text":"","title":"3. Login to your HPC account by typing: ssh youraccount@talon3.hpc.unt.edu, press Enter and you\u2019ll be asked to insert password as bellow:"},{"location":"tutorials/getting_started/#4-fill-in-with-your-password-hit-enter-and-you-are-in","text":"","title":"4. Fill in with your password, hit Enter and you are in!"},{"location":"tutorials/getting_started/#5-environment-setup","text":"Before running any kind of code, you need to make sure you have the right environment set up. For this, type \u201cmodule avail\u201d and press Enter. This will give you something like in the following picture where you can check all modules that are available on HPC.","title":"5. Environment setup"},{"location":"tutorials/getting_started/#6-lookup-module","text":"Look for the module you need. Let\u2019s say you want to run your code using Python 3.6.5. I will type: module load python/3.6.5 because that\u2019s the name in the list of modules available (see the bottom of the picture above)","title":"6. Lookup module"},{"location":"tutorials/getting_started/#7-upload-file-to-talon","text":"Now that it\u2019s all set up you only need to upload your file containing the python code","title":"7. Upload file to Talon"},{"location":"tutorials/getting_started/#8-access-user-folder","text":"You need to access your user folder from HPC. To do that type in: cd /storage/scratch2/youraccount press Enter Hint: if you want to check what subfolders or files you have here you can type ll (small L\u2019s)","title":"8. Access user folder"},{"location":"tutorials/getting_started/#9-create-project-folder","text":"I will create a folder for this project I want to run just to be organized mkdir functions you can replace word functions with the desired name for your folder Hint: avoid spaces, don\u2019t give names like \u201cProject 1\u201d, instead you can say \u201cProject_1\u201d after creating the folder I used the ll or ls command to check my files and folders from where I am at the moment","title":"9. Create project folder"},{"location":"tutorials/getting_started/#10-access-your-new-folder","text":"Access your new folder where the magic will happen: cd functions/","title":"10. Access your new folder"},{"location":"tutorials/getting_started/#11-copy-local-to-hpc","text":"Now, you want to copy your .py file from local computer on talon in the new folder created. For this, open a new terminal window without closing this one. Use the command cd to access the path of the file you want to transfer. For example: cd Downloads/ But, if you are not sure where you are at you can type \u201cls\u201d first: I know my file is in Downloads folder so I\u2019ll go there by typing \u201ccd Downloads/\u201d I used again the \u201cls\u201d command to see all the files in the folder, but what I am interested in is functions_comparison.py Now, to transfer this file I\u2019ll do the following: scp functions_comparison.py lp0348@talon3.hpc.unt.edu :/storage/scratch2/lp0348/functions Notice the command is: scp filename youraccount@talon3.hpc.unt.edu :/the/folder/path/on/talon After introducing your HPC account password you should see something like: Notice there\u2019s a percentage indicating the transfer progress Go back to the previous terminal window you were working on. You can check if the file was transferred: Here it is! Step 12. Time to create a batch file What is a batch file? \u2013 It is a script containing code running details related to resources you want to allocate from HPC. Detailed specifications on each line you can find here: https://github.com/gmihaila/unt_hpc/tree/master/job_batch For our example we will use a basic configuration. #!/bin/bash #SBATCH -J job_name #SBATCH -o output_job.o%j ## <- can be left like that (creates a file with the output) #SBATCH -e error_job.e%j. ## <- can be left like that (creates a file with the errors occurred) #SBATCH -p public #SBATCH --qos general #SBATCH -N 1 #SBATCH -n 1 #SBATCH -C c6320 module load python python test.py ## <- name of the .py code file We need to create a file .job where you put the above information. In this tutorial we will use VIM: The command: vim name_of_your_job.job After running this line you should see: This automatically created the batch file .job and opened it. In order to insert the content we discussed above, first hit \u201ci\u201d button from the keyboard. Notice the bottom changed to \u201c\u2014INSERT\u2014\u201d Now, you can type each line from scratch or you can paste it directly. But, if you are using Linux Ctrl+Shift+V would be the command to paste some content; if you are using Mac Command+V as usual will do the work. It might be easier for the beginning to write the batch content elsewhere and then just copy-paste the final commands. To save and quit the batch file: press \u201cEsc\u201d then type \u201c:wq\u201d Now \u201cEnter\u201d and will bring you back to the folder.","title":"11. Copy local to HPC"},{"location":"tutorials/getting_started/#13-run-the-job","text":"The command line is: sbatch job_name.job You\u2019d probably get the message: It\u2019s done! You submitted a job that runs the py code on HPC. If you want to check your jobs you can use the command: squeue -u your_account If all your jobs are done already then squeue -u your_account command will give you the above output.","title":"13. Run the job!"},{"location":"tutorials/getting_started/#from-windows","text":"","title":"From Windows"},{"location":"tutorials/tutorials/","text":"Tutorials and Worksops Talon HPC \u00b6 This is where we will post tutorials and workshops regarding Talon HPC Please check our tutorials dedicated GitHub Repository: https://github.com/UNT-RITS/Tutorials Introduction course to R \u00b6 Created by Jonathan Starkweather from Data Science & Analytics Presentation Using Rstudio on Talon3 \u00b6 A video tutorial from Rich Herrington from Data Science & Analystics Presentation Find New Sentiments in Text Data \u00b6 march 03, 2020 | University of North Texas DP F223 Presented by George Mihaila Using text embedding and clustering methods to find new sentiments in text data. Use imdb movie reviews dataset and see how positive-negative sentiments can be used to find other sentiments. Use Tensorflow 2, Word Embeddings, Contextual Embeddings. We\u2019ll do clustering and find best number of cluster in data we don\u2019t know anything about. We\u2019ll do pretty plots to interpret results! We\u2019ll do all this on our UNT Talon Supercomputer! Presentation | Materials Intro to Word Embeddings - NLP Tools on Talon \u00b6 may 2, 2019 | University of North Texas DP F223 Presented by George Mihaila Natural Language Processing on Talon HPC Ever wondered or curious about natural language processing? How do you load a text file, how do you \u2018clean\u2019 it, how do you make use of it? We will learn all that and at the end you will build a Neural Network model that can predict a sentiment of a Movie Review. All this on Talon! Presentation | Materials Using Python and Jupyter Notebooks on Talon \u00b6 may 1, 2019 | University of North Texas DP F223 Presented by George Mihaila Hands on work on Talon UNT HPC This workshop will go over some basic instruction about Python and Jupyter Notebooks. We will learn how to see what libraries we have available and how to install your own.Learn how to create a virtual environment, and most exciting build a simple Neural Network model to predict diabetes! All this on Talon! Presentation | Materials Machine Learning - Neural Networks on Talon \u00b6 november 30, 2018 | University of North Texas GAB 535 Presented by George Mihaila What are Neural Networks, and why everyone is talking about them? This workshop is meant to give a general overview of why Neural Networks and Deep Learning are getting so much attention and help shed some light on how they work and what applications are possible because of them. Presentation |","title":"Tutorials"},{"location":"tutorials/tutorials/#tutorials-and-worksops-talon-hpc","text":"This is where we will post tutorials and workshops regarding Talon HPC Please check our tutorials dedicated GitHub Repository: https://github.com/UNT-RITS/Tutorials","title":"Tutorials and Worksops Talon HPC"},{"location":"tutorials/tutorials/#introduction-course-to-r","text":"Created by Jonathan Starkweather from Data Science & Analytics Presentation","title":"Introduction course to R"},{"location":"tutorials/tutorials/#using-rstudio-on-talon3","text":"A video tutorial from Rich Herrington from Data Science & Analystics Presentation","title":"Using Rstudio on Talon3"},{"location":"tutorials/tutorials/#find-new-sentiments-in-text-data","text":"march 03, 2020 | University of North Texas DP F223 Presented by George Mihaila Using text embedding and clustering methods to find new sentiments in text data. Use imdb movie reviews dataset and see how positive-negative sentiments can be used to find other sentiments. Use Tensorflow 2, Word Embeddings, Contextual Embeddings. We\u2019ll do clustering and find best number of cluster in data we don\u2019t know anything about. We\u2019ll do pretty plots to interpret results! We\u2019ll do all this on our UNT Talon Supercomputer! Presentation | Materials","title":"Find New Sentiments in Text Data"},{"location":"tutorials/tutorials/#intro-to-word-embeddings-nlp-tools-on-talon","text":"may 2, 2019 | University of North Texas DP F223 Presented by George Mihaila Natural Language Processing on Talon HPC Ever wondered or curious about natural language processing? How do you load a text file, how do you \u2018clean\u2019 it, how do you make use of it? We will learn all that and at the end you will build a Neural Network model that can predict a sentiment of a Movie Review. All this on Talon! Presentation | Materials","title":"Intro to Word Embeddings - NLP Tools on Talon"},{"location":"tutorials/tutorials/#using-python-and-jupyter-notebooks-on-talon","text":"may 1, 2019 | University of North Texas DP F223 Presented by George Mihaila Hands on work on Talon UNT HPC This workshop will go over some basic instruction about Python and Jupyter Notebooks. We will learn how to see what libraries we have available and how to install your own.Learn how to create a virtual environment, and most exciting build a simple Neural Network model to predict diabetes! All this on Talon! Presentation | Materials","title":"Using Python and Jupyter Notebooks on Talon"},{"location":"tutorials/tutorials/#machine-learning-neural-networks-on-talon","text":"november 30, 2018 | University of North Texas GAB 535 Presented by George Mihaila What are Neural Networks, and why everyone is talking about them? This workshop is meant to give a general overview of why Neural Networks and Deep Learning are getting so much attention and help shed some light on how they work and what applications are possible because of them. Presentation |","title":"Machine Learning - Neural Networks on Talon"},{"location":"user_guide/getting_started/","text":"Get started on Talon \u00b6 How to run a short .py code on HPC (Talon 3) \u00b6 For Mac OS/Linux Ubuntu \u00b6 1. Request an HPC account: \u00b6 If you already have one, you are good to go; If you don\u2019t have one, please visit https://hpc.unt.edu/account-request ; Read carefully and make sure you have all information requested: When you are ready, go to https://hpc.unt.edu/user ; Login in with your EUID (6 characters: initials+4 digits); Fill in the form and wait for our email. Make sure you are connected to the UNT VPN: Instructions can be found here https://itservices.cas.unt.edu/services/accounts-servers/articles/cisco-anyconnect-mobility-client-vpn 2. Open terminal/command prompt \u00b6 3. Login to your HPC account by typing: ssh youraccount@talon3.hpc.unt.edu , press Enter and you\u2019ll be asked to insert password as bellow: \u00b6 4. Fill in with your password, hit Enter and you are in! \u00b6 5. Environment setup \u00b6 Before running any kind of code, you need to make sure you have the right environment set up. For this, type \u201cmodule avail\u201d and press Enter. This will give you something like in the following picture where you can check all modules that are available on HPC. 6. Lookup module \u00b6 Look for the module you need. Let\u2019s say you want to run your code using Python 3.6.5. I will type: module load python/3.6.5 because that\u2019s the name in the list of modules available (see the bottom of the picture above) 7. Upload file to Talon \u00b6 Now that it\u2019s all set up you only need to upload your file containing the python code 8. Access user folder \u00b6 You need to access your user folder from HPC. To do that type in: cd /storage/scratch2/youraccount press Enter Hint: if you want to check what subfolders or files you have here you can type ll (small L\u2019s) 9. Create project folder \u00b6 I will create a folder for this project I want to run just to be organized mkdir functions you can replace word functions with the desired name for your folder Hint: avoid spaces, don\u2019t give names like \u201cProject 1\u201d, instead you can say \u201cProject_1\u201d after creating the folder I used the ll or ls command to check my files and folders from where I am at the moment 10. Access your new folder \u00b6 Access your new folder where the magic will happen: cd functions/ 11. Copy local to HPC \u00b6 Now, you want to copy your .py file from local computer on talon in the new folder created. For this, open a new terminal window without closing this one. Use the command cd to access the path of the file you want to transfer. For example: cd Downloads/ But, if you are not sure where you are at you can type \u201cls\u201d first: I know my file is in Downloads folder so I\u2019ll go there by typing \u201ccd Downloads/\u201d I used again the \u201cls\u201d command to see all the files in the folder, but what I am interested in is functions_comparison.py Now, to transfer this file I\u2019ll do the following: scp functions_comparison.py lp0348@talon3.hpc.unt.edu :/storage/scratch2/lp0348/functions Notice the command is: scp filename youraccount@talon3.hpc.unt.edu :/the/folder/path/on/talon After introducing your HPC account password you should see something like: Notice there\u2019s a percentage indicating the transfer progress Go back to the previous terminal window you were working on. You can check if the file was transferred: Here it is! Step 12. Time to create a batch file What is a batch file? \u2013 It is a script containing code running details related to resources you want to allocate from HPC. Detailed specifications on each line you can find here: https://github.com/gmihaila/unt_hpc/tree/master/job_batch For our example we will use a basic configuration. #!/bin/bash #SBATCH -J job_name #SBATCH -o output_job.o%j ## <- can be left like that (creates a file with the output) #SBATCH -e error_job.e%j. ## <- can be left like that (creates a file with the errors occurred) #SBATCH -p public #SBATCH --qos general #SBATCH -N 1 #SBATCH -n 1 #SBATCH -C c6320 module load python python test.py ## <- name of the .py code file We need to create a file .job where you put the above information. In this tutorial we will use VIM: The command: vim name_of_your_job.job After running this line you should see: This automatically created the batch file .job and opened it. In order to insert the content we discussed above, first hit \u201ci\u201d button from the keyboard. Notice the bottom changed to \u201c\u2014INSERT\u2014\u201d Now, you can type each line from scratch or you can paste it directly. But, if you are using Linux Ctrl+Shift+V would be the command to paste some content; if you are using Mac Command+V as usual will do the work. It might be easier for the beginning to write the batch content elsewhere and then just copy-paste the final commands. To save and quit the batch file: press \u201cEsc\u201d then type \u201c:wq\u201d Now \u201cEnter\u201d and will bring you back to the folder. 13. Run the job! \u00b6 The command line is: sbatch job_name.job You\u2019d probably get the message: It\u2019s done! You submitted a job that runs the py code on HPC. If you want to check your jobs you can use the command: squeue -u your_account If all your jobs are done already then squeue -u your_account command will give you the above output. From Windows \u00b6","title":"Get started on Talon"},{"location":"user_guide/getting_started/#get-started-on-talon","text":"","title":"Get started on Talon"},{"location":"user_guide/getting_started/#how-to-run-a-short-py-code-on-hpc-talon-3","text":"","title":"How to run a short .py code on HPC (Talon 3)"},{"location":"user_guide/getting_started/#for-mac-oslinux-ubuntu","text":"","title":"For  Mac OS/Linux Ubuntu"},{"location":"user_guide/getting_started/#1-request-an-hpc-account","text":"If you already have one, you are good to go; If you don\u2019t have one, please visit https://hpc.unt.edu/account-request ; Read carefully and make sure you have all information requested: When you are ready, go to https://hpc.unt.edu/user ; Login in with your EUID (6 characters: initials+4 digits); Fill in the form and wait for our email. Make sure you are connected to the UNT VPN: Instructions can be found here https://itservices.cas.unt.edu/services/accounts-servers/articles/cisco-anyconnect-mobility-client-vpn","title":"1. Request an HPC account:"},{"location":"user_guide/getting_started/#2-open-terminalcommand-prompt","text":"","title":"2. Open terminal/command prompt"},{"location":"user_guide/getting_started/#3-login-to-your-hpc-account-by-typing-ssh-youraccounttalon3hpcuntedu-press-enter-and-youll-be-asked-to-insert-password-as-bellow","text":"","title":"3. Login to your HPC account by typing: ssh youraccount@talon3.hpc.unt.edu, press Enter and you\u2019ll be asked to insert password as bellow:"},{"location":"user_guide/getting_started/#4-fill-in-with-your-password-hit-enter-and-you-are-in","text":"","title":"4. Fill in with your password, hit Enter and you are in!"},{"location":"user_guide/getting_started/#5-environment-setup","text":"Before running any kind of code, you need to make sure you have the right environment set up. For this, type \u201cmodule avail\u201d and press Enter. This will give you something like in the following picture where you can check all modules that are available on HPC.","title":"5. Environment setup"},{"location":"user_guide/getting_started/#6-lookup-module","text":"Look for the module you need. Let\u2019s say you want to run your code using Python 3.6.5. I will type: module load python/3.6.5 because that\u2019s the name in the list of modules available (see the bottom of the picture above)","title":"6. Lookup module"},{"location":"user_guide/getting_started/#7-upload-file-to-talon","text":"Now that it\u2019s all set up you only need to upload your file containing the python code","title":"7. Upload file to Talon"},{"location":"user_guide/getting_started/#8-access-user-folder","text":"You need to access your user folder from HPC. To do that type in: cd /storage/scratch2/youraccount press Enter Hint: if you want to check what subfolders or files you have here you can type ll (small L\u2019s)","title":"8. Access user folder"},{"location":"user_guide/getting_started/#9-create-project-folder","text":"I will create a folder for this project I want to run just to be organized mkdir functions you can replace word functions with the desired name for your folder Hint: avoid spaces, don\u2019t give names like \u201cProject 1\u201d, instead you can say \u201cProject_1\u201d after creating the folder I used the ll or ls command to check my files and folders from where I am at the moment","title":"9. Create project folder"},{"location":"user_guide/getting_started/#10-access-your-new-folder","text":"Access your new folder where the magic will happen: cd functions/","title":"10. Access your new folder"},{"location":"user_guide/getting_started/#11-copy-local-to-hpc","text":"Now, you want to copy your .py file from local computer on talon in the new folder created. For this, open a new terminal window without closing this one. Use the command cd to access the path of the file you want to transfer. For example: cd Downloads/ But, if you are not sure where you are at you can type \u201cls\u201d first: I know my file is in Downloads folder so I\u2019ll go there by typing \u201ccd Downloads/\u201d I used again the \u201cls\u201d command to see all the files in the folder, but what I am interested in is functions_comparison.py Now, to transfer this file I\u2019ll do the following: scp functions_comparison.py lp0348@talon3.hpc.unt.edu :/storage/scratch2/lp0348/functions Notice the command is: scp filename youraccount@talon3.hpc.unt.edu :/the/folder/path/on/talon After introducing your HPC account password you should see something like: Notice there\u2019s a percentage indicating the transfer progress Go back to the previous terminal window you were working on. You can check if the file was transferred: Here it is! Step 12. Time to create a batch file What is a batch file? \u2013 It is a script containing code running details related to resources you want to allocate from HPC. Detailed specifications on each line you can find here: https://github.com/gmihaila/unt_hpc/tree/master/job_batch For our example we will use a basic configuration. #!/bin/bash #SBATCH -J job_name #SBATCH -o output_job.o%j ## <- can be left like that (creates a file with the output) #SBATCH -e error_job.e%j. ## <- can be left like that (creates a file with the errors occurred) #SBATCH -p public #SBATCH --qos general #SBATCH -N 1 #SBATCH -n 1 #SBATCH -C c6320 module load python python test.py ## <- name of the .py code file We need to create a file .job where you put the above information. In this tutorial we will use VIM: The command: vim name_of_your_job.job After running this line you should see: This automatically created the batch file .job and opened it. In order to insert the content we discussed above, first hit \u201ci\u201d button from the keyboard. Notice the bottom changed to \u201c\u2014INSERT\u2014\u201d Now, you can type each line from scratch or you can paste it directly. But, if you are using Linux Ctrl+Shift+V would be the command to paste some content; if you are using Mac Command+V as usual will do the work. It might be easier for the beginning to write the batch content elsewhere and then just copy-paste the final commands. To save and quit the batch file: press \u201cEsc\u201d then type \u201c:wq\u201d Now \u201cEnter\u201d and will bring you back to the folder.","title":"11. Copy local to HPC"},{"location":"user_guide/getting_started/#13-run-the-job","text":"The command line is: sbatch job_name.job You\u2019d probably get the message: It\u2019s done! You submitted a job that runs the py code on HPC. If you want to check your jobs you can use the command: squeue -u your_account If all your jobs are done already then squeue -u your_account command will give you the above output.","title":"13. Run the job!"},{"location":"user_guide/getting_started/#from-windows","text":"","title":"From Windows"},{"location":"user_guide/user_guide/","text":"Talon 3 User Guide \u00b6 All User Guide informaton can be found on our website: https://hpc.unt.edu/userguide Here are a few helpful links: Accessing Talon 3 \u00b6 Logging into Talon 3 Changing password Transferring files Running Jobs and Job Submission \u00b6 Talon 3 job submission Interactive sessions","title":"Talon 3 User Guide"},{"location":"user_guide/user_guide/#talon-3-user-guide","text":"All User Guide informaton can be found on our website: https://hpc.unt.edu/userguide Here are a few helpful links:","title":"Talon 3 User Guide"},{"location":"user_guide/user_guide/#accessing-talon-3","text":"Logging into Talon 3 Changing password Transferring files","title":"Accessing Talon 3"},{"location":"user_guide/user_guide/#running-jobs-and-job-submission","text":"Talon 3 job submission Interactive sessions","title":"Running Jobs and Job Submission"},{"location":"userinfo/files/","text":"Talon3 file management \u00b6 Transferring files \u00b6 To/From Windows computer \u00b6 To transfer files with a Windows machine, you will need an SFTP/SCP client software such as WinSCP or Cyberduck installed. When you download a client application, the application will require you to connect to Talon 3 (talon3.hpc.unt.edu). Once logged in, you can use the file transfer window within the program to drag and drop files between local (your personal computer) and remote (Talon 3) machines. To/From Mac/Linux computer \u00b6 To transfer files to and from a cluster on a Linux/Mac machine, you may use the terminal application and the secure copy (scp) command or secure file transfer protocol (sftp). The following is an example of uploading a file foo.f to Talon 3 from your local machine: scp foo.f talon3.hpc.unt.edu You can download files from Talon3 to your local machine. scp talon3.hpc.unt.edu:/home/EUID/foo.f ./ To recursively copy an entire directory foo from inside directory project_foo in your home directory on Talon 3 to the current working directory on localhost, use the following command: scp -rp talon3.hpc.unt.edu:/home/EUID/project_foo ./ Using rclone with OneDrive \u00b6 Warning We are still working on updating our docs! Check back for more information! Please email SciComp-Support@unt.edu for your questions!","title":"Managing Files"},{"location":"userinfo/files/#talon3-file-management","text":"","title":"Talon3 file management"},{"location":"userinfo/files/#transferring-files","text":"","title":"Transferring files"},{"location":"userinfo/files/#tofrom-windows-computer","text":"To transfer files with a Windows machine, you will need an SFTP/SCP client software such as WinSCP or Cyberduck installed. When you download a client application, the application will require you to connect to Talon 3 (talon3.hpc.unt.edu). Once logged in, you can use the file transfer window within the program to drag and drop files between local (your personal computer) and remote (Talon 3) machines.","title":"To/From Windows computer"},{"location":"userinfo/files/#tofrom-maclinux-computer","text":"To transfer files to and from a cluster on a Linux/Mac machine, you may use the terminal application and the secure copy (scp) command or secure file transfer protocol (sftp). The following is an example of uploading a file foo.f to Talon 3 from your local machine: scp foo.f talon3.hpc.unt.edu You can download files from Talon3 to your local machine. scp talon3.hpc.unt.edu:/home/EUID/foo.f ./ To recursively copy an entire directory foo from inside directory project_foo in your home directory on Talon 3 to the current working directory on localhost, use the following command: scp -rp talon3.hpc.unt.edu:/home/EUID/project_foo ./","title":"To/From Mac/Linux computer"},{"location":"userinfo/files/#using-rclone-with-onedrive","text":"Warning We are still working on updating our docs! Check back for more information! Please email SciComp-Support@unt.edu for your questions!","title":"Using rclone with OneDrive"},{"location":"userinfo/login/","text":"Logging into Talon3 \u00b6 Intro \u00b6 Talon3 is a HPC computing cluster that users remotly access. To login, you will need to make a remote connection to one of Talon 3\u2019s login nodes. User MUST : Have a ACTIVE Talon account that has been approved by NTSC Here is more information about requesting a Talon3 account Be connected to the UNT network This includes computers connected to the UNT Campus LAN, Eaglenet Wi-Fi, and the UNT VPN Network Talon3 can be access with the following methods Secure Shell (SSH) terminal Command-line interface Most common inferface to Talon3 Rstudio server Access to the Rsudio environment Jupyter Using Jupyter Notebooks and Hub within the Talon3 topology PyCharm Use PyCharm IDE on Talon3 Off Campus access \u00b6 Warning Important information about Off Campus access Connecting to Talon3 is only possible if your computer is in the UNT network. Examples are computers connected to the UNT Campus LAN and Eaglenet/UNT Wi-Fi. If you are off campus, you may connect to Talon3 using the UNT Campus VPN. Connecting to the UNT VPN requires an active AMS UNT EUID. This is accomplished by installing the Cisco AnyConnect VPN Client. More information about installing the VPN Client can be found here and here . Once you connect to the UNT VPN (located at vpn.unt.edu ) you can then access Talon via SSH terminal or the other ways to remotly connect. Login information \u00b6 In order to login to talon you will have to use SSH command on your personal computer\u2019s temrinal : Talon3 password \u00b6 When you account is first created, your password is your UNTID number by default and is only active for two days. You MUST login and change it by running the command passwd and enter in a new password. After this, your password will be active for 180 days. An email will be send a week before expiration as a reminder. If you forget your password or it becomes expired, please consult SciComp-Support@unt.edu . $ ssh euid123@talon3.hpc.unt.edu Then it will ask for you password. To make things easier you cna have it save the password and even create a shortcut so you can speed things up: Setup alias on you SSH connection \u00b6 Make sure you are in your home directory: $ cd ~ Check if you have a folder names .ssh $ ls -a If you don\u2019t you have to create one: $ mkdir .ssh Now you need to create a file named config. Use Nano or Vim or any other editor you are comfortable with: $ vim ~/.ssh/config If, for example, you want to create a shortcut to ssh on Talon3, your config file should look like: 1 ```bash Host t3 HostName talon3.hpc.unt.edu User euid123 Port 22 Where t3 (it can be any word you want) is the shortcut you will use instead of typing: bash $ ssh euid123@talon3.hpc.unt.edu ``` You will type: $ ssh t3 Save SSH password \u00b6 Now you have a shortcut, but you will still need to enter your password every time. In order to save your passwords you will have to create a keygen file: $ ssh-keygen When prompted with this: Enter file in which to save the key ( /home/your_user/.ssh/id_rsa ) : Just press Enter to save the id_ras in the path. Then you will be prompted with: Enter passphrase ( empty for no passphrase ) : Type Enter again if you don\u2019t want to type any password when ssh Enter same passphrase again: Hit Enter again! You should see: Your identification has been saved in /home/your_user/.ssh/id_rsa. Your public key has been saved in /home/you_user/.ssh/id_rsa.pub. Now you can start saving passwords on you SSH: $ ssh-copy-id euid123@talon3.hpc.unt.edu or if you have your shortcut $ ssh-copy-id t3 You will be prompted to enter password. It will logout, and you should see: Number of key ( s ) added: 1 Now try logging into the machine, with: \"ssh 't3'\" and check to make sure that only the key ( s ) you wanted were added. Congratulations! Now every time you need to login to Talon you just need to type ssh t3 and your good to go! When you want to copy files using SCP \u00b6 If you are not familiar with SCP you can find a nice example here Now since you setup your ssh shortcut to be t3 and the password saved, when you use SCP it will be a lot easier! Just type scp path/to/file/my_file t3:. It will automatically login and enter the password for you! For more details on how to transfer files to Talon use Transferring files","title":"Logging into Talon3"},{"location":"userinfo/login/#logging-into-talon3","text":"","title":"Logging into Talon3"},{"location":"userinfo/login/#intro","text":"Talon3 is a HPC computing cluster that users remotly access. To login, you will need to make a remote connection to one of Talon 3\u2019s login nodes. User MUST : Have a ACTIVE Talon account that has been approved by NTSC Here is more information about requesting a Talon3 account Be connected to the UNT network This includes computers connected to the UNT Campus LAN, Eaglenet Wi-Fi, and the UNT VPN Network Talon3 can be access with the following methods Secure Shell (SSH) terminal Command-line interface Most common inferface to Talon3 Rstudio server Access to the Rsudio environment Jupyter Using Jupyter Notebooks and Hub within the Talon3 topology PyCharm Use PyCharm IDE on Talon3","title":"Intro"},{"location":"userinfo/login/#off-campus-access","text":"Warning Important information about Off Campus access Connecting to Talon3 is only possible if your computer is in the UNT network. Examples are computers connected to the UNT Campus LAN and Eaglenet/UNT Wi-Fi. If you are off campus, you may connect to Talon3 using the UNT Campus VPN. Connecting to the UNT VPN requires an active AMS UNT EUID. This is accomplished by installing the Cisco AnyConnect VPN Client. More information about installing the VPN Client can be found here and here . Once you connect to the UNT VPN (located at vpn.unt.edu ) you can then access Talon via SSH terminal or the other ways to remotly connect.","title":"Off Campus access"},{"location":"userinfo/login/#login-information","text":"In order to login to talon you will have to use SSH command on your personal computer\u2019s temrinal :","title":"Login information"},{"location":"userinfo/login/#talon3-password","text":"When you account is first created, your password is your UNTID number by default and is only active for two days. You MUST login and change it by running the command passwd and enter in a new password. After this, your password will be active for 180 days. An email will be send a week before expiration as a reminder. If you forget your password or it becomes expired, please consult SciComp-Support@unt.edu . $ ssh euid123@talon3.hpc.unt.edu Then it will ask for you password. To make things easier you cna have it save the password and even create a shortcut so you can speed things up:","title":"Talon3 password"},{"location":"userinfo/login/#setup-alias-on-you-ssh-connection","text":"Make sure you are in your home directory: $ cd ~ Check if you have a folder names .ssh $ ls -a If you don\u2019t you have to create one: $ mkdir .ssh Now you need to create a file named config. Use Nano or Vim or any other editor you are comfortable with: $ vim ~/.ssh/config If, for example, you want to create a shortcut to ssh on Talon3, your config file should look like: 1 ```bash Host t3 HostName talon3.hpc.unt.edu User euid123 Port 22 Where t3 (it can be any word you want) is the shortcut you will use instead of typing: bash $ ssh euid123@talon3.hpc.unt.edu ``` You will type: $ ssh t3","title":"Setup alias on you SSH connection"},{"location":"userinfo/login/#save-ssh-password","text":"Now you have a shortcut, but you will still need to enter your password every time. In order to save your passwords you will have to create a keygen file: $ ssh-keygen When prompted with this: Enter file in which to save the key ( /home/your_user/.ssh/id_rsa ) : Just press Enter to save the id_ras in the path. Then you will be prompted with: Enter passphrase ( empty for no passphrase ) : Type Enter again if you don\u2019t want to type any password when ssh Enter same passphrase again: Hit Enter again! You should see: Your identification has been saved in /home/your_user/.ssh/id_rsa. Your public key has been saved in /home/you_user/.ssh/id_rsa.pub. Now you can start saving passwords on you SSH: $ ssh-copy-id euid123@talon3.hpc.unt.edu or if you have your shortcut $ ssh-copy-id t3 You will be prompted to enter password. It will logout, and you should see: Number of key ( s ) added: 1 Now try logging into the machine, with: \"ssh 't3'\" and check to make sure that only the key ( s ) you wanted were added. Congratulations! Now every time you need to login to Talon you just need to type ssh t3 and your good to go!","title":"Save SSH password"},{"location":"userinfo/login/#when-you-want-to-copy-files-using-scp","text":"If you are not familiar with SCP you can find a nice example here Now since you setup your ssh shortcut to be t3 and the password saved, when you use SCP it will be a lot easier! Just type scp path/to/file/my_file t3:. It will automatically login and enter the password for you! For more details on how to transfer files to Talon use Transferring files","title":"When you want to copy files using SCP"},{"location":"userinfo/modules/","text":"Environment Modules \u00b6 Talon 3 uses LMOD to manage your environment. It handles setting up access to the software packages, libraries, and other utilities. This includes compilers, system libraries, and math libraries that are typically needed to build scientific codes. If you need to use a certain application that the HPC staff maintains, you will need to load the appropriate module. Loading the module will modify your environment variables, such as $PATH and $LD_LIBRARY_PATH, that are needed to run the certain application. To see the list of all available modules, run the command: module avail To see which modules are already loaded: module list To add a module for the current session module load <module_name> where is the name of the module you wish to load. For example, if you need to use the Intel compilers, you can run module load intel/compilers/18.0.3-AVX This will load the Intel version 18.0.3 compilers. To configure a module so that it will be loaded into your environment at login: module initadd <module_name> To remove a module: module remove <module_name>","title":"Environment Modules"},{"location":"userinfo/modules/#environment-modules","text":"Talon 3 uses LMOD to manage your environment. It handles setting up access to the software packages, libraries, and other utilities. This includes compilers, system libraries, and math libraries that are typically needed to build scientific codes. If you need to use a certain application that the HPC staff maintains, you will need to load the appropriate module. Loading the module will modify your environment variables, such as $PATH and $LD_LIBRARY_PATH, that are needed to run the certain application. To see the list of all available modules, run the command: module avail To see which modules are already loaded: module list To add a module for the current session module load <module_name> where is the name of the module you wish to load. For example, if you need to use the Intel compilers, you can run module load intel/compilers/18.0.3-AVX This will load the Intel version 18.0.3 compilers. To configure a module so that it will be loaded into your environment at login: module initadd <module_name> To remove a module: module remove <module_name>","title":"Environment Modules"},{"location":"userinfo/slurm/","text":"Slurm Job Scheduler \u00b6 Overview \u00b6 In an HPC cluster, the users\u2019 tasks to be done on compute nodes are controlled by a batch queuing system. Queuing systems manage job requests (shell scripts generally referred to as jobs) submitted by all users on Talon 3. In other words, to get your computations done by the cluster, you must submit a job request to a specific batch queue. The scheduler will assign your job to a compute node in the order determined by the policy on that queue and the availability of an idle compute node. Currently, Talon 3 resources have several policies in place to help guarantee fair resource utilization from all users. The SLURM Workload Manger is userd to control how jobs are dispatched on the compute nodes. Basic Information about Slurm \u00b6 The Slurm Workload Manager (formally known as Simple Linux Utility for Resource Management or SLURM), or Slurm, is a free and open-source job scheduler for Linux and Unix-like kernels, used by many of the world\u2019s supercomputers and computer clusters. It provides three key functions. First, it allocates exclusive and/or non-exclusive access to resources (computer nodes) to users for some duration of time so they can perform work. Second, it provides a framework for starting, executing, and monitoring work (typically a parallel job such as MPI) on a set of allocated nodes. Finally, it arbitrates contention for resources by managing a queue of pending jobs. Slurm is the workload manager on about 60% of the TOP500 supercomputers, including Tianhe-2 that, until 2016, was the world\u2019s fastest computer. Slurm uses a best fit algorithm based on Hilbert curve scheduling or fat tree network topology in order to optimize locality of task assignments on parallel computers. Slurm Tutorials and Commands \u00b6 A Quick-Start Guide for those unfamiliar with Slurm can be found here Slurm Tutorial Videos can be found here for additional information Talon3 Job Submission \u00b6 Partitions \u00b6 Currently on Talon3, there are 4 partitions for users to submit their jobs. Partition Node Type Max Nodes Max Duration Details preproduction R420 22 See QOS Available to any Talon3 user production C6320 22 **See QOS ** Available to production users bigmem R720 2 1 weeks Available to any Talon3 user gpu R730 3 1 weeks Available to any Talon3 user Users have access the production partition if their allocation reaches 300,000 CPU hours within the current and previous FY OR they have an active external funding source. If you require more resources, you may submit a support ticket to SciComp-Support@unt.edu and the Talon3 admin staff will evaaluate your request. QOS \u00b6 The Quality of Service, QOS, sets the priority of the job to the queuing system. There are three QOS\u2019s under the preproduction and production partitions. For the bigmem and gpu partition, you do NOT need to specify QOS. QOS Description debug Two hour limit two compute nodes Highest priority general 72 hour limit 22 compute nodes Medium priority large 3 week limit 22 compute nodes lowest priority Common Slurm information \u00b6 The following list frequently used SLURM commands Slurm Command Description sbatch script.job submit a job squeue jobid display job status squeue -u $USER display status of user\u2019s jobs scancel jobid canncel a job scontrol update modify a pending job The following lists command SLURM variables that you can use in your job scripts SLURM Variable Description $SLURM_SUBMIT_DIR current working directory of the submitting client $SLURM_JOB_ID unique identifier assigned when the job was submitted $SLURM_NTASKS number of CPUs in use by a parallel job $SLURM_NNODES number of hosts in use by a parallel job $SLURM_ARRAY_TASK_ID index number of the current array job task $SLURM_JOB_CPUS_PER_NODE number of CPU cores per node $SLURM_JOB_NAME Name of JOB SLURM Example Commands \u00b6 Submitting a job sbatch slurm.job Where slurm.job is the name of the job script List all current jobs that a user has in the queue squeue -u $USER Get job details scontrol show job 106 where 106 is the JOBID number Kill a job that is in the queue scancel 106 where 106 is t he JOBID number of the job you wan to be killed Hold a job in the queue scontrol hold 106 Release a held job in the queue scontrol release 106 The SLURM Job Script \u00b6 The job script is a file that has the SLURM information for sbatch. The following table list command sbatch options Option Argument Description -p parition_name submits to a certain [arition_name -J job_name defines the name of the job -o output_name defines file name to direct standard output from job Default slurm-JOBID.out -e error_name defines file name to direct error output Defalut output_name \u2013qos QOS_name defines the requested QOS \u2013exclusive N/A sets exclusive job, not allowing other jobs to share on compute node -t time set up the wall time limit for job in hh ss -N num_nodes Definies the total number of compute nodes requested -n num_cores Definies the total number of cpu tasks requested \u2013ntasks-per-node num_cpu_pernode Definies the number of cpu tasks PER node \u2013mail-user user_email sets up email notification \u2013mail-type mail_type Emails users when mail_type is: begin A email will set when job starts end A email will set when job ends Example job Scripts \u00b6 Simple serial job script example #!/bin/bash ###################################### # Example of a SLURM job script for Talon3 # Job Name: Sample_Job # Number of cores: 1 # Number of nodes: 1 # QOS: general # Run time: 12 hrs ###################################### #SBATCH -J Sample_Job #SBATCH -o Sample_job.o%j #SBATCH -p preproduction #SBATCH --qos general #SBATCH -N 1 #SBATCH -n 1 #SBATCH -t 12:00:00 ### Loading modules module load intel/compilers/18.0.1 ./a.out > outfile.out Simple parallel MPI job script example When submitting a MPI job, make sure you load the module and use mpirun to launch your application #!/bin/bash ###################################### # Example of a SLURM job script for Talon3 # Job Name: Sample_Job # Number of cores: 28 # Number of nodes: 1 # QOS: general # Run time: 12 hrs ###################################### #SBATCH -J Sample_Job #SBATCH -o Sample_job.o%j #SBATCH -p preproduction #SBATCH --qos general #SBATCH -N 1 #SBATCH -n 16 #SBATCH --ntasks-per-node 16 #SBATCH -t 12:00:00 ### Loading modules module load PackageEnv/intel17.0.4_gcc8.1.0_MKL_IMPI_AVX ### Use mpirun to run parallel jobs mpirun ./a.out > outfile.out Larger MPI job script example #!/bin/bash ###################################### # Example of a SLURM job script for Talon3 # Job Name: Sample_Job # Number of cores: 112 # Number of nodes: 4 # QOS: general # Run time: 12 hrs ###################################### #SBATCH -J Sample_Job #SBATCH -o Sample_job.o%j #SBATCH -p preproduction #SBATCH --qos general #SBATCH -N 4 #SBATCH -n 64 #SBATCH --ntasks-per-node 16 #SBATCH -t 12:00:00 ### Loading modules module load PackageEnv/intel17.0.4_gcc8.1.0_MKL_IMPI_AVX ## Use mpirun for MPI jobs mpirun ./a.out > outfile.out Big Memory job script example #!/bin/bash ###################################### # Example of a SLURM job script for Talon3 # Job Name: Sample_Job # Number of MPI tasks: 32 # Number of nodes: 1 # Run time: 12 hrs ###################################### #SBATCH -J Sample_Job #SBATCH -o Sample_job.o%j #SBATCH -p bigmem #SBATCH -N 1 #SBATCH --ntasks-per-node 32 #SBATCH -t 12:00:00 ### Loading modules module load PackageEnv/intel17.0.4_gcc8.1.0_MKL_IMPI_AVX mpirun ./a.out > outfile.out CUDA parallel GPU job script example #!/bin/bash ###################################### # Example of a SLURM job script for Talon3 # Job Name: Sample_Job # Number of devices(GPUs): 2 # Number of nodes: 1 # QOS: general # Run time: 12 hrs ###################################### #SBATCH -J Sample_Job #SBATCH --ntasks=1 #SBATCH --gres=gpu:2 #SBATCH -t 12:00:00 #SBATCH -p gpu ### execute code ./a.out -numdevices=2 Interactive sessions \u00b6 Interactive job sessions can be used on Talon if you need to compile or test software. Interactive jobs will start a command line session on a compute node. If you have to run large tasks or processes, running an interactive job will allow you to run these tasks without using the login nodes. An example command of starting an interactive session is shown below: $ srun -p preproduction --qos debug -N 1 -n 16 -t 2:00:00 --pty bash srun: job 55692 queued and waiting for resources srun: job 55692 has been allocated resources [c64-6-32 ~]$ python test.py # Run python on compute node [c64-6-32 ~]$ exit # Exit interactive job This launches an interactive job session and launches a bash shell to a compute node. From there, you can execute software and shell commands that would otherwise not be allowed on the Talon login nodes.","title":"SLURM queuing system"},{"location":"userinfo/slurm/#slurm-job-scheduler","text":"","title":"Slurm Job Scheduler"},{"location":"userinfo/slurm/#overview","text":"In an HPC cluster, the users\u2019 tasks to be done on compute nodes are controlled by a batch queuing system. Queuing systems manage job requests (shell scripts generally referred to as jobs) submitted by all users on Talon 3. In other words, to get your computations done by the cluster, you must submit a job request to a specific batch queue. The scheduler will assign your job to a compute node in the order determined by the policy on that queue and the availability of an idle compute node. Currently, Talon 3 resources have several policies in place to help guarantee fair resource utilization from all users. The SLURM Workload Manger is userd to control how jobs are dispatched on the compute nodes.","title":"Overview"},{"location":"userinfo/slurm/#basic-information-about-slurm","text":"The Slurm Workload Manager (formally known as Simple Linux Utility for Resource Management or SLURM), or Slurm, is a free and open-source job scheduler for Linux and Unix-like kernels, used by many of the world\u2019s supercomputers and computer clusters. It provides three key functions. First, it allocates exclusive and/or non-exclusive access to resources (computer nodes) to users for some duration of time so they can perform work. Second, it provides a framework for starting, executing, and monitoring work (typically a parallel job such as MPI) on a set of allocated nodes. Finally, it arbitrates contention for resources by managing a queue of pending jobs. Slurm is the workload manager on about 60% of the TOP500 supercomputers, including Tianhe-2 that, until 2016, was the world\u2019s fastest computer. Slurm uses a best fit algorithm based on Hilbert curve scheduling or fat tree network topology in order to optimize locality of task assignments on parallel computers.","title":"Basic Information about Slurm"},{"location":"userinfo/slurm/#slurm-tutorials-and-commands","text":"A Quick-Start Guide for those unfamiliar with Slurm can be found here Slurm Tutorial Videos can be found here for additional information","title":"Slurm Tutorials and Commands"},{"location":"userinfo/slurm/#talon3-job-submission","text":"","title":"Talon3 Job Submission"},{"location":"userinfo/slurm/#partitions","text":"Currently on Talon3, there are 4 partitions for users to submit their jobs. Partition Node Type Max Nodes Max Duration Details preproduction R420 22 See QOS Available to any Talon3 user production C6320 22 **See QOS ** Available to production users bigmem R720 2 1 weeks Available to any Talon3 user gpu R730 3 1 weeks Available to any Talon3 user Users have access the production partition if their allocation reaches 300,000 CPU hours within the current and previous FY OR they have an active external funding source. If you require more resources, you may submit a support ticket to SciComp-Support@unt.edu and the Talon3 admin staff will evaaluate your request.","title":"Partitions"},{"location":"userinfo/slurm/#qos","text":"The Quality of Service, QOS, sets the priority of the job to the queuing system. There are three QOS\u2019s under the preproduction and production partitions. For the bigmem and gpu partition, you do NOT need to specify QOS. QOS Description debug Two hour limit two compute nodes Highest priority general 72 hour limit 22 compute nodes Medium priority large 3 week limit 22 compute nodes lowest priority","title":"QOS"},{"location":"userinfo/slurm/#common-slurm-information","text":"The following list frequently used SLURM commands Slurm Command Description sbatch script.job submit a job squeue jobid display job status squeue -u $USER display status of user\u2019s jobs scancel jobid canncel a job scontrol update modify a pending job The following lists command SLURM variables that you can use in your job scripts SLURM Variable Description $SLURM_SUBMIT_DIR current working directory of the submitting client $SLURM_JOB_ID unique identifier assigned when the job was submitted $SLURM_NTASKS number of CPUs in use by a parallel job $SLURM_NNODES number of hosts in use by a parallel job $SLURM_ARRAY_TASK_ID index number of the current array job task $SLURM_JOB_CPUS_PER_NODE number of CPU cores per node $SLURM_JOB_NAME Name of JOB","title":"Common Slurm information"},{"location":"userinfo/slurm/#slurm-example-commands","text":"Submitting a job sbatch slurm.job Where slurm.job is the name of the job script List all current jobs that a user has in the queue squeue -u $USER Get job details scontrol show job 106 where 106 is the JOBID number Kill a job that is in the queue scancel 106 where 106 is t he JOBID number of the job you wan to be killed Hold a job in the queue scontrol hold 106 Release a held job in the queue scontrol release 106","title":"SLURM Example Commands"},{"location":"userinfo/slurm/#the-slurm-job-script","text":"The job script is a file that has the SLURM information for sbatch. The following table list command sbatch options Option Argument Description -p parition_name submits to a certain [arition_name -J job_name defines the name of the job -o output_name defines file name to direct standard output from job Default slurm-JOBID.out -e error_name defines file name to direct error output Defalut output_name \u2013qos QOS_name defines the requested QOS \u2013exclusive N/A sets exclusive job, not allowing other jobs to share on compute node -t time set up the wall time limit for job in hh ss -N num_nodes Definies the total number of compute nodes requested -n num_cores Definies the total number of cpu tasks requested \u2013ntasks-per-node num_cpu_pernode Definies the number of cpu tasks PER node \u2013mail-user user_email sets up email notification \u2013mail-type mail_type Emails users when mail_type is: begin A email will set when job starts end A email will set when job ends","title":"The SLURM Job Script"},{"location":"userinfo/slurm/#example-job-scripts","text":"Simple serial job script example #!/bin/bash ###################################### # Example of a SLURM job script for Talon3 # Job Name: Sample_Job # Number of cores: 1 # Number of nodes: 1 # QOS: general # Run time: 12 hrs ###################################### #SBATCH -J Sample_Job #SBATCH -o Sample_job.o%j #SBATCH -p preproduction #SBATCH --qos general #SBATCH -N 1 #SBATCH -n 1 #SBATCH -t 12:00:00 ### Loading modules module load intel/compilers/18.0.1 ./a.out > outfile.out Simple parallel MPI job script example When submitting a MPI job, make sure you load the module and use mpirun to launch your application #!/bin/bash ###################################### # Example of a SLURM job script for Talon3 # Job Name: Sample_Job # Number of cores: 28 # Number of nodes: 1 # QOS: general # Run time: 12 hrs ###################################### #SBATCH -J Sample_Job #SBATCH -o Sample_job.o%j #SBATCH -p preproduction #SBATCH --qos general #SBATCH -N 1 #SBATCH -n 16 #SBATCH --ntasks-per-node 16 #SBATCH -t 12:00:00 ### Loading modules module load PackageEnv/intel17.0.4_gcc8.1.0_MKL_IMPI_AVX ### Use mpirun to run parallel jobs mpirun ./a.out > outfile.out Larger MPI job script example #!/bin/bash ###################################### # Example of a SLURM job script for Talon3 # Job Name: Sample_Job # Number of cores: 112 # Number of nodes: 4 # QOS: general # Run time: 12 hrs ###################################### #SBATCH -J Sample_Job #SBATCH -o Sample_job.o%j #SBATCH -p preproduction #SBATCH --qos general #SBATCH -N 4 #SBATCH -n 64 #SBATCH --ntasks-per-node 16 #SBATCH -t 12:00:00 ### Loading modules module load PackageEnv/intel17.0.4_gcc8.1.0_MKL_IMPI_AVX ## Use mpirun for MPI jobs mpirun ./a.out > outfile.out Big Memory job script example #!/bin/bash ###################################### # Example of a SLURM job script for Talon3 # Job Name: Sample_Job # Number of MPI tasks: 32 # Number of nodes: 1 # Run time: 12 hrs ###################################### #SBATCH -J Sample_Job #SBATCH -o Sample_job.o%j #SBATCH -p bigmem #SBATCH -N 1 #SBATCH --ntasks-per-node 32 #SBATCH -t 12:00:00 ### Loading modules module load PackageEnv/intel17.0.4_gcc8.1.0_MKL_IMPI_AVX mpirun ./a.out > outfile.out CUDA parallel GPU job script example #!/bin/bash ###################################### # Example of a SLURM job script for Talon3 # Job Name: Sample_Job # Number of devices(GPUs): 2 # Number of nodes: 1 # QOS: general # Run time: 12 hrs ###################################### #SBATCH -J Sample_Job #SBATCH --ntasks=1 #SBATCH --gres=gpu:2 #SBATCH -t 12:00:00 #SBATCH -p gpu ### execute code ./a.out -numdevices=2","title":"Example job Scripts"},{"location":"userinfo/slurm/#interactive-sessions","text":"Interactive job sessions can be used on Talon if you need to compile or test software. Interactive jobs will start a command line session on a compute node. If you have to run large tasks or processes, running an interactive job will allow you to run these tasks without using the login nodes. An example command of starting an interactive session is shown below: $ srun -p preproduction --qos debug -N 1 -n 16 -t 2:00:00 --pty bash srun: job 55692 queued and waiting for resources srun: job 55692 has been allocated resources [c64-6-32 ~]$ python test.py # Run python on compute node [c64-6-32 ~]$ exit # Exit interactive job This launches an interactive job session and launches a bash shell to a compute node. From there, you can execute software and shell commands that would otherwise not be allowed on the Talon login nodes.","title":"Interactive sessions"}]}